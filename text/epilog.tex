\chapter*{Conclusion}
\addcontentsline{toc}{chapter}{Conclusion}

We confirmed that cache-oblivious B-trees are competitive with more common
data structures in main memory. A hand-optimized B-tree does better on some
operations, but at the cost of optimizing for one memory level in advance.
As we have seen on the Firefox recordings, parameters good for one size of
dictionaries may also do not so well in other settings.

Our implementations of $k$-splay trees and $k$-forests were outperformed
by traditional splay trees. In our opinion, $k$-forests alone are not
a practical replacement for simple splay trees. However, a more optimized
implementation of $k$-splaying (with removed heap allocation and with top-down
$k$-splaying) might reach parity with splay trees or slightly beat them
on large dictionaries.

Experiments on data recorded from Mozilla Firefox suggest that splay trees,
cuckoo hash tables and red-black trees do well on most dictionaries as they are
practically used. A particularly interesting conclusion that can be drawn from
table~\ref{tab:firefox-results} is that hashing with linear probing may
be (somewhat counterintuitively) slower than splay trees.
Larger databases that require order queries also seem to be best served
by splay trees, followed by cache-oblivious \mbox{B-trees} and cache-aware
\mbox{B-trees}.

\section*{Suggestions for further research}
The dictionary problem is well-explored, including structures for efficient
in particular models of computation and practical libraries for real computers.
The space-time limitations of this thesis unfortunately did not permit us to
completely survey all of them.

The performance of \textsc{Find}s seems to be also dependent on
the rate of success. Unsuccessful accesses could potentially be filtered out
early by adding a~\emph{Bloom~filter} before the dictionary. We suggest
measuring the effect in a practical settings. Our measurements from Mozilla
Firefox may be biased towards unsuccessful \textsc{Find}s due to the way we
filtered out unparseable lines.

In the Firefox recording, we found that eithre cuckoo hash tables, splay trees,
or red-black trees were always the winning data structure. Analyzing the
recorded access pattern may allow predicting which data structure would win.
Based on such analysis, it may be possible to develop a data structure that
would switch its internal representation based on the previously seen access
pattern. Another approach may be collecting the access pattern first, and
then automatically choosing the best data structure at compile time.

\subsection{B-trees and their variants}
Our implementation of \mbox{B-trees} was largely unoptimized. In particular,
we used a linear scan when looking within nodes. On small nodes, this is
not very different from the speed of binary search, but since we found
that nodes larger than $64\,\text{B}$ cache lines did very well, binary search
within these large nodes might make a difference.

Many variants of B-trees were developed in the settings of on-disk databases.
One of them, \emph{B*-trees} \cite{bstar}, use some extra bookkeeping to
maintain fuller nodes. This may also be a useful optimization in main memory.

\subsection{Cache-oblivious B-trees}
The cache-oblivious B-tree shows promise: without any tuning, it almost
matched a hand-optimized B-tree. By choosing the tuning constants (i.e.,\ the
minimum and maximum density and piece sizes) more carefully, we might obtain
a better match. Another possible point of improvement may be the van Emde Boas
layout. For example, we might choose a simpler and more predictable
representation for small trees that fit within a single cache line
(e.g.,\ the BFS layout).

\subsection{Other self-adjusting structures}
We believe benchmarking various non-traditional self-adjusting structures,
like Tango trees from \cite{tango} or multi-splay trees from
\cite{multisplay-trees}, could give interesting results. Splay trees are very
performant on practical access patterns, but their worst-case $\O(N)$ behavior
limits their usefulness in time-sensitive systems. Some self-adjusting
structures provide better worst-case access times (e.g.,\ $\O(\log N)$ in
multi-splay trees). On the other hand, the code for splay trees would
likely be considerably simpler, so the complexity of operations might outweight
time saved by fewer cache misses in the worst case.

\subsection{Other approaches}
Several data structures based on tries have also been proposed for the
dictionary problem. One example are \emph{y-fast tries} \cite{y-fast},
which allow \textsc{Find}s in time $\O(1)$ and updates and predecessor/successor
queries in time $\O(\log\log |U|)$, where $U$~is the key universe. They require
$\O(N \log\log |U|)$ memory.
The RAM model is required by y-fast tries -- they assume that RAM operations
can be performed on keys in constant time.
The original motivation for developing y-fast tries was lowering the memory
requirements of van Emde Boas queues ($\O(|U|)$) while keeping the same time
for operations.
% TODO: Try to find some practical numbers.

\emph{Judy arrays} are highly optimized dictionaries developed at
Hewlett-Packard \cite{judy-shop-manual, judy-patent}.
HP sources claim that Judy arrays are very efficient in practice.
The data structure is based on sparse 256-ary tries with implementation tricks
that compress the representation.
Unfortunately, we found the existing documentation highly complex and sometimes
incomplete. On the other hand, an open-source implementation is available
at \url{http://judy.sourceforge.net/}, so it should be possible to independently
analyze the data structure and possibly to improve upon it.
