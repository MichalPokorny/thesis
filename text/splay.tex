\chapter{Splay trees}
\label{chapter:splay}
Splay trees are a well-known variant of binary search trees introduced
by \cite{splay} that adjust their structure with every operation, including
lookups. Splay tree operations take $\O(\log N)$ amortized time, which is
similar to common balanced binary search trees, like AVL trees or
red-black trees.

However, unlike balanced search trees, splay trees adjust their structure
according to the access pattern, and a family of theorems proves that
various non-random access patterns are fast on splay trees.
According to the \textit{static optimality theorem}, the running time
of a sufficiently long sequence of \textsc{Find} operations on a splay tree
is within a constant factor of an optimal static search tree.
The \textit{working set theorem} proves that the speed of accessing keys in
any small \textit{working set} depends only on the logarithm of the size
of the working set.
Finally, the \textit{dynamic finger theorem} claims that splay trees are also
fast when accessing keys that are numerically close to recently accessed keys.
All of these results are implied by the unproven \textit{dynamic optimality
conjecture}, which proposes that splay trees are dynamically optimal binary
search trees: if a binary search tree optimally tuned for the access sequence
$S$ needs $\mathrm{OPT}(S)$ operations to perform the access sequence $S$,
splay trees are presumed to only need $\O(\mathrm{OPT}(x))$ steps.

TODO: disadvantage - slow individual operations, lots of local adjustments

As binary search trees, splay trees are composed of nodes, each of which
contains one key and its associated value. A node has up to two children,
denoted \textit{left} and \textit{right}.
The left subtree of a node with key $k$ contains only keys smaller than $k$ and,
symetrically, the right subtree contains only keys larger than $k$.

% To search for a key $K$, we use binary search as in usual trees:

Splay trees are maintained using a heuristic called \textit{splaying}, which
moves a specified node to the root of the tree by performing a sequence
of edge rotations along the path from the root to this note.
Rotations cost $\O(1)$ time each and they preserve the left-to-right order
of nodes in the binary search tree. Rotations are defined in figure
\ref{fig:rotation}.

\begin{figure}
\centering
\tikzset{
  splay_node/.style = {align=center, inner sep=0pt, text centered, circle,
	  font=\sffamily, draw=black, text width=1.2em},
  triangle/.style = {regular polygon, regular polygon sides=3, scale=0.8},
  splay_placeholder/.style = {align=center, inner sep=0pt, text centered,
	  triangle, font=\sffamily, draw=black, text width=1.2em},
}
\begin{tikzpicture}[]
\node [splay_node] at (0, 0) (x-before) {x}
child { node [splay_node] (y-before) {y}
	child { node [splay_placeholder] (Ph1-b) {1} }
	child { node [splay_placeholder] (Ph2-b) {2} }
}
child { node [splay_placeholder] (Ph3-b) {3} };

\node [fit=(x-before) (y-before) (Ph1-b) (Ph2-b) (Ph3-b)] (Before) {};

\node [splay_node] at (6, 0) (y-after) {y}
child { node [splay_placeholder] (Ph1-a) {1} }
child { node [splay_node] (x-after) {x}
	child { node [splay_placeholder] (Ph2-a) {2} }
	child { node [splay_placeholder] (Ph3-a) {3} }
};

\node [fit=(x-after) (y-after) (Ph1-a) (Ph2-a) (Ph3-a)] (After) {};
\path[->] ($(Before.east) + (+.5,+.5)$) edge node [above] {rotate right} ($(After.west)
+ (-.5,+.5)$);
\path[->] ($(After.west) - (+.5,+.5)$) edge node [below] {rotate left} ($(Before.east)
- (-.5,+.5)$);
\end{tikzpicture}
\caption{Left and right rotation of the edge between $x$ and $y$.}
\label{fig:rotation}
\end{figure}

To splay a node $x$, we perform the following \textit{splaying step}
until $x$ is the root:

Splaying step:
Denote the splayed node by $x$, its parent by $p$ and its grandparent by $g$.
\begin{itemize}
\item[Case 1 (\textit{zig}):] If $p$ is the root, rotate the $px$ edge. (This case is terminal.)
\item[Case 2 (\textit{zig-zig}):] If $p$ is not the root and $x$ and
		$p$ are both left or both right children, rotate
		the edge $gp$, then rotate the edge $px$.
\item[Case 3 (\textit{zig-zag}):] If $p$ is not the root and $x$ is a left child and $p$
		is a right child (or vice versa), rotate the edge $px$ and
		then rotate the edge now joining $g$ with $x$.
\end{itemize}

\begin{figure}
\centering
TODO: figure jako splay-trees.pdf (see page 5)
\caption{The three cases of a splaying step up to left-right symmetry --
	zig, zig-zig and zig-zag.}
%\label{fig:splay-step}
\end{figure}

Splaying a node $x$ of depth $d$ takes $\Theta(d)$ time, which is the same
as the time to access $x$ from the root.
% Splaying reduces the depth of every node along the access path by roughly
% one half. TODO: fakt? TODO: figure

To \textsc{Find} a key in a splay tree, we use binary search as in any
binary search tree: in every node, we compare its key $k$ with $K$, and
if $k \neq K$, we continue to the left subtree or to the right subtree.
If the subtree we would like to descend into is empty, the search is aborted,
since $K$ is not present in the tree.
In splay trees, after the search finishes (successfully or unsuccessfully),
we splay the last visited node.
An \textsc{Insert} is similar - we find the right place for the new node,
insert it, and finally splay it up.

\textsc{Delete}s are slightly more complex. We first find and splay the node $x$
we need to delete. If $x$ has one child after splaying, we delete it and replace
it by its only child. One possible way to handle the case of two children is
finding the rightmost node $x^-$ in $x$'s left subtree and splaying it just
below $x$. By definition, $x^-$ is the largest in $x$'s left subtree, so it
must have no right child after splaying below $x$. We delete $x$ and link
its right subtree below $x^-$.

The amortized complexity of the splay operation is $\O(\log N)$,
so \textsc{Find}s and updates on splay trees also have this complexity.

The performance of the \textsc{Splay} operation can be analyzed by assigning
a fixed positive weight $w(K)$ to every key $K$. Let us denote the subtree
rooted at a node $x$ as $T[x]$ and the key stored in $x$ as $T(x)$.
Define the size $s(x)$ of a node $x$ as $\sum_{y\in T[x]} w(T(y))$
and the rank $r(x)$ as $\log s(x)$.
Let us also denote the keys stored in $K$ as $K_1,\ldots K_N$ in sorted order.
For amortization, let us define the potential $\Phi$ of the tree to be the sum
of the ranks of all its nodes. Since every rotation can be performed in $\O(1)$
pointer assignments, we will measure the time to splay a node in the number
of rotations performed (or 1 if no rotations are needed).

% TODO: to je lemma
\begin{lemma}
The amortized time to splay a node $x$ in a tree with root
$t$ is $3(r(t)-r(x))+1 = \O(\log(s(t)/s(x)))$.
\end{lemma}
\begin{proof}
If $x=t$, there were no rotations and the bound is immediate.
Thus suppose there is at least one rotation. Consider any splaying step.
Let $s$ and $s'$, $r$ and $r'$ denote the size and rank functions just before
and just after the step, respectively. We show that the amortized time for
the step is at most $3(r'(x)-r(x))+1$ in case 1 and at most $3(r'(x)-r(x))$
in case 2 or case 3. Let $p$ be the original parent of $x$ and $g$ the original
grandparent of $x$ (if it exists).

\begin{itemize}
\item[Case 1 (\textit{zig}):]
	The only needed rotation of the $px$ edge may only change the rank of
	$x$ and $p$, so the amortized time for this step is
	$1 + r'(x) + r'(p) - r(x) - r(p)$.
	Because $r(p)\geq r'(p)$, the amortized time is $\leq 1+r'(x)-r(x)$.
	Finally, since $r'(x)\geq r(x)$, the amortized time is also
	$\leq 1+3(r'(x)-r(x))$.

\item[Case 2 (\textit{zig-zig}):]
	The two rotations may change the rank of $x$, $p$ and $g$,
	so the amortized time for the zig-zig step is
	$2+r'(x)+r'(y)+r'(z)-r(x)-r(y)-r(z)$. The zig-zig step
	moves $x$ to the original position of $g$, so $r'(x)=r(g)$.
	Before the step, $x$ was $p$'s child, and after the step,
	$p$ was $x$'s child, so $r(x)\leq r(p)$ and $r'(p)\leq r'(x)$.
	Thus the amortized time for this step is at most $2+r'(x)+r'(g)-2r(x)$.
	We claim that this is at most $3(r'(x)-r(x))$, that is, that
	$2r'(x)-r(x)-r(g)\geq 2$.

	$2r'(x)-r(x)-r'(g)=-\log\frac{s(x)}{s'(x)}-\log\frac{s'(g)}{s'(x)}$.
	We have $s(x)+s'(g)\leq s'(x)$.

	If $a,b\geq 0$ and $a+b\leq 1$, $\log a + \log b$ is maximized
	by setting $a = b = \frac{1}{2}$, which yields $\log a + \log b = -2$.
	Thus the inequality above is correct and the amortized time is
	at most $3(r'(x)-r(x))$.

\item[Case 3 (\textit{zig-zag}):]
	The amortized time of the zig-zag step is
	$2+r'(x)+r'(p)+r'(g)-r(x)-r(p)-r(g)$. $r'(x)=r(g)$ and $r(x)\leq r(p)$,
	so the time is $\leq 2+r'(p)+r'(g)-2r(x)$.
	We claim that this is at most $2(r'(x)-r(x))$, i.e.
	$2r'(x)-r'(p)-r'(g)\geq 2$. This can be proven
	similarly to case 2 from the inequality $s'(p)+s'(g)\leq s'(x)$.
	Thus the amortized time for a zig-zag step is at most
	$2(r'(x)-r(x))\leq 3(r'(x)-r(x))$.
\end{itemize}

By telescoping the sum of amortized time for all performed steps until
$x$ becomes the root $t$, we get an upper bound of $3(r'(t)-r(x))+1$
for the entire splaying operation.
\end{proof}

Note that over any sequence of $M$ splay operations
% TODO: key vs. node assignment: w(i)
the total potential $\Phi$ may only drop by up to $\sum_{i=1}^N \log(W/w(K_i))$
where $W=\sum_{i=1}^N w(K_i)$, since the size of the node containing $K_i$
must be between $w(K_i)$ and $W$.

Assigning different weights $w$ yields several basic results.

\begin{theorem}[Balance Theorem]
A sequence of $M$ \textsc{Find}s on a splay tree with $N$ nodes takes time
$\O((M+N)\log N+M)$.
\end{theorem}
\begin{proof}
Assign $w(K_i) = \frac{1}{N}$ to every key $K_i$. The amortized
access time for any key is at most $3\log N+1$. We have $W=1$, so by
the previous observation the potential drops by at most $N\log N$ over
the access sequence. Thus the time to perform the access sequence is at most
$(3\log N+1)M + N\log N = \O((M+N)\log N + M)$.
\end{proof}

TODO: relate Balance theorem and costs of inserts and deletes

For any key $K_i$, let $q(K_i)$ be the number of accesses to $K_i$ in an access
sequence.
\begin{theorem}[Static Optimality Theorem]
If every key is accessed at least once, the the total access time is
$\O(M + \sum_{i=1}^N q(K_i)\log\frac{M}{q(K_i)})$.
\end{theorem}
\begin{proof}
Assign a weight of $q(i)/M$ to every key $i$. For any key $i$, its
amortized time per access is $3(r(t)-r(x))+1=-3r(x)+1=-3\log(s(x))+1\leq
	-3\log(w(x))=O(\log(M/q(i)))$.
Since $W=1$, the net potential drop over the sequence is at most
$\sum_{i=1}{N}\log(M/q(i))$. The result follows.
\end{proof}

Let us denote the indices of accessed keys as $a_1, \ldots a_M$
in the order they are accessed, where $a_i \in \{1,\ldots N\}$.
TODO: This is pretty much wrong.

\begin{theorem}[Static Finger Theorem]
Fix any key $i_f$. Then the total access time is $\O(N\log N + M +
\sum_{j=1}^M \log(|a_j-f|+1))$.
\end{theorem}
\begin{proof}
	% TODO: proof missing here
\end{proof}

For any access $j\in\{1,\ldots M\}$, let $t(j)$ be the number of different
keys accessed since the last access of key $i_{a_j}$ or since the
beginning of the access sequence.
\begin{theorem}[Working Set Theorem]
The total access time is $\O(N\log N + M + \sum_{j=1}^M\log(t(j)+1))$.
\end{theorem}
\begin{proof}
	% TODO: proof missing here
\end{proof}

The balance theorem states that on a sufficiently long sequence, lookups in
the splay tree are as efficient as in any balanced search tree. The static
optimality theorem implies that the splay tree is as efficient as any fixed
search tree, including the optimum tree for the given sequence, since
it is known that any sequence of accesses on a binary search tree
takes time at least $\Omega(M+\sum_{i=1}^N q(i)\log(M/q(i)))$.
% TODO: cite: ABRAMSON, N. Information Theory and Coding. McGraw-Hill, New York, 1983.

% TODO: Nope. That's dynamic finger theorem.
The static finger theorem can be interpreted to mean that accessing keys
whose value is close to a recently accessed value is fast.
Finally, the working set theorem implies that most recently
accessed keys (the "working set") are cheap to access, which
makes splay trees behave well when the access pattern exhibits temporal
locality.

Further known results about splay trees include the \textit{dynamic finger
theorem} proven in \cite{dynamic-finger-1} and \cite{dynamic-finger-2}:
\begin{theorem}[Dynamic Finger Theorem]
The cost of performing an access sequence $a_1,\ldots a_M$ is
$\O(N+M+\sum_{i=2}^M \log(|a_i-a_{i-1}| + 1))$.
\end{theorem}

TODO: dynamic optimality conjecture

It is well-known that splay trees are $\O(\log N)$-competitive.
TODO: cite 'Dynamic Optimality - Almost' by Demaine-Harmon-Iacono-Patrascu
(Tango trees)
TODO: how to splay for log log N-competitiveness: Demaine et al.; Derryberry et
al.; Georgakopoulos


\section{Implementation}
% TODO: top-down splay

\section{???}
A drawback of splay trees is that accessing the $\O(\log N)$ amortized
nodes on the access path may incur an expensive cache miss for every node --
splay trees make no effort to exploit the memory hierarchy.
In contrast, lookups in B-trees and cache-oblivious B-trees
only cause up to $\Theta(\log_B N)$ cache misses, which is a multiplicative
improvement of $\log B$.

On the other hand, splaying automatically adjusts the structure of the splay
tree to speed up access to recently or frequently accessed nodes. By the
working set theorem, accessing a small set of keys is always fast, no matter
how big the splay tree is. While B-trees and cache-oblivious B-trees are
also faster on smaller working sets, the complexity of any access remains
$\O(\log_B N)$, so accessing a small working set is slower on larger
dictionaries.

\cite{alternatives-to-splay-trees} introduces a data structure based
on $k$-forests, which we describe in chapter \ref{chapter:forest}.
This structure has the \textit{unified property}, which is a strong
generalization of the dynamic finger property and the working set property.
It is not known whether splay trees also have the unified property.
Additionally, the new data structure improves upon splay trees by guaranteeing
a worst-case access time of $\O(\log N)$. Unfortunately, the structure
(as described by the author) is static and highly complex: each node of
the structure stores 21 pointers.
