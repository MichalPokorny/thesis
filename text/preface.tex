\chapter*{Introduction}
\addcontentsline{toc}{chapter}{Introduction}

It is a well-known fact that there is a widening gap between
the performance of CPUs and memory. To optimize memory accesses,
modern computers include several small and fast caches between the CPU and
main memory. However, if programs access data mostly randomly, I/O can become
the performance bottleneck, even for programs running only in main memory.

Standard models of computation only measure the performance of algorithms
in the number of CPU instructions executed and the number of memory cells used.
These models aren't well suited for environments where different memory cells
may have vastly different access speeds, for example depending on which caches
currently hold the particular memory cell. The \emph{external memory model}
is a simple extension of the RAM model that is better suited for analyzing
algorithms in memory hierarchies. This model adds a \emph{block transfer}
measure, which is the number of \emph{blocks} transferred between a fast cache
and a slow \emph{external memory}.
While the original motivation was developing fast algorithms for data stored on
hard drives, the model can also be used to reason about boundaries between
internal memory and caches.

\emph{Cache-oblivious algorithms} are a class of external memory algorithms
that perform equally well given any cache size and block size. Thanks to this
property, they need little tuning for efficient implementation.
Modern CPU architectures perform various optimizations that can change
the effective size of a block, so tuning an algorithm by setting a good block
size might be difficult.
The external memory model and cache-oblivious algorithms are introduced in more
detail in chapter~\ref{chapter:models}.

This thesis explores the performance implications of memory hierarchies
on data structures implementing dictionaries or sorted dictionaries.
Especially unsorted dictionaries are very useful in practice, as illustrated
by modern programming languages, which frequently reserve a built-in
data type for unsorted dictionaries (e.g.,\ \texttt{std::unordered\_map} in
\Cpp, \texttt{dict} in Python, or hashes in Perl).

Dictionaries maintain a set of $N$ key-value pairs with distinct keys.
Denote the set of stored keys by $K$.
The \textsc{Find} operation find the value associated with a~given key, or
it reports that no such value exists. \textsc{Insert} inserts a new
key-value pair and \textsc{Delete} removes a key-value pair given its key.
Sorted dictionaries additionally maintain an ordering on the keys and they
allow \textsc{FindNext} and \textsc{FindPrevious} operations.
Given a key $k$, which may or may not be present in the dictionary,
\textsc{FindNext} and \textsc{FindPrevious} find the closest larger or smaller
key present in the dictionary.

We implemented and benchmarked several common and uncommon data structures
for sorted and unsorted dictionaries. Our choice of data structures was
intended to sample simple dictionaries for RAM, cache-friendly dictionaries
for external memory and self-adjusting structures.
A high-level description of our implementation and benchmarks is presented
in chapter \ref{chapter:implementation}. The results are discussed in
chapter \ref{chapter:results}.

The most common data structure implementing an unsorted dictionary is a~hash
table. We describe some variants of hash tables in chapter \ref{chapter:hashing}.
Most hash tables have expected $\O(1)$ time for all operations, which
makes them quite practical. On the other hand, there is no simple way to
implement sorted dictionaries efficiently via hash tables.

Ordered dictionaries are usually maintained in search trees.
AVL trees, red-black trees and B-trees are examples of \emph{balanced search
trees}: a design invariant maintains their height low. The speeds
of \textsc{Find}, \textsc{Insert} and \textsc{Delete} mostly depend
on the number of nodes touched by the operations, which is $\Theta(h)$, where
$h$ is the height of the tree. AVL trees and red-black trees are binary, so
$h=\Theta(\log N)$. B-trees store $\Theta(b)$ keys per node, where $b\geq 2$,
and they maintain all leaves at the same depth, so operations on B-trees
touch $\Theta(\log_b N)$ nodes.

Splay trees are binary search trees with a self-adjusting rule, but splay
trees are not balanced, as this rule does not guarantee a small height in all
cases. The structure of a splay tree depends not only on the sequence of
\textsc{Insert}s and \textsc{Delete}s, but also on executed \textsc{Find}s.
The self-adjusting rule (\emph{splaying}) puts recently accessed nodes
near the root, so \textsc{Find}s on frequently or recently accessed keys
are faster than in balanced trees. On the other hand, splaying can put
the splay tree in a~degenerate state, in which accessing an unlucky node
touches $\Theta(N)$ nodes. Fortunately, the amortized number of node
reads per \textsc{Find} turns out to be $\O(\log N)$.

Splay trees are good at storing nonuniformly accessed dictionaries, but
the expected number of nodes read to \textsc{Find} a key is generally a factor
of $\Theta(\log b)$ higher than in B-trees. We briefly introduce two
self-adjusting data structures designed for both exploiting non-uniform
access patterns and to perform less I/O: \emph{$k$-splay trees} in chapter
\ref{chapter:ksplay} and \emph{$k$-forests} in chapter \ref{chapter:kforest}.

In chapter \ref{chapter:cob}, we describe \emph{cache-oblivious B-trees}.
The number of memory operations for cache-oblivious B-tree operations
is within a constant factor of the bounds given by a B-tree optimally tuned
for the memory hierarchy. In practice, we found the performance of
cache-oblivious B-trees quite competitive, especially on large datasets
with uniform access patterns.

The chapters on hashing and cache-oblivious B-trees are based on the Advanced
Data Structures course as taught by Erik Demaine on MIT in spring 2012.
Lecture videos and other materials are available through MIT OpenCourseWare.\footnote{%
\url{http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-851-advanced-data-structures-spring-2012/index.htm}}

\section*{Notation and conventions}
In this thesis, logarithms are in base 2 unless stated otherwise.
Chapter \ref{chapter:cob} uses the \emph{hyperfloor} operation
$\lhfloor x\rhfloor$, which is defined as $2^{\lfloor\log x\rfloor}$.

We denote bitwise XOR as $\oplus$, and similarly $\bigoplus_{i=1}^N
a_i=a_1\oplus\ldots\oplus a_N$.

This thesis contains several references to memory sizes. The base units of
memory size are traditionally powers of two to simplify some operations on
addresses.
When discussing memory, we shall follow this convention, so
$1\ \mathrm{kB}=1024\ \mathrm{B}$, $1\ \mathrm{MB}=1024\ \mathrm{kB}$,
and so on.
