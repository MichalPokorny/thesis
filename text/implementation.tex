\chapter{Implementation}
\label{chapter:implementation}
To measure the performance of various data structures, we implemented them
in the C programming language (using the C11 revision).
We chose this language for several reasons:
\begin{itemize}
\item The language is low-level, which enables a~high degree of tuning by hand.
	Our data structures are also not slowed down by common features of
	high-level languages, such as garbage collection.
\item C toolchains are mature and they can be expected to optimize code well.
\item The C language is very portable and most other languages can
	call C functions. This enables our data structures to be potentially
	easily used in other projects.
\end{itemize}

Our code was developed using the GNU C Compiler (version 4.9.2) on Linux.
We used the Git version control system, with our repository hosted at
\url{https://github.com/MichalPokorny/thesis}. The code includes a~test suite,
which tests implementation details of particular data structures as well
as general fuzz tests for checking that implemented data structures behave
like proper dictionaries. We used Travis CI (\url{https://travis-ci.org/})
to automatically test our code after submission.

All dictionaries assume 64-bit keys and values (represented as the
\texttt{uint64\_t} type). It would be easy to allow keys and values of arbitrary
size, but such a~choice would be likely to slow down operations on
the data structures, because a~range of compiler optimizations would
be impossible in code assuming general lengths. For example, copying
keys and values would need to be a~general loop or
\texttt{memcpy}/\texttt{memmove} call, while assuming a~constant key/value
size of 64 bits lets us copy in one CPU instruction. Using the \Cpp language,
we could potentially use templates to make our data structures both general
and optimized. If we are willing to sacrifice some code style, we could
make the C code behave similarly by moving the implementation into a~header
file, which could be configured to generate a~general data structure
tuned to specific parameters prior to inclusion, as in the following example:
\begin{lstlisting}
struct my_value {
	uint64_t stuff[32];
};
#define IMPLEMENTATION_PREFIX myimpl
#define IMPLEMENTATION_KEY_TYPE uint64_t
#define IMPLEMENTATION_VALUE_TYPE struct my_value
#include "btree/general.impl.h"

void usage() {
	dict* btree;
	dict_init(&btree, &dict_myimpl_btree);
	// ...
}
\end{lstlisting}
For example, the LibUCW library (\url{http://www.ucw.cz/libucw/})
uses this pattern for generic binomial heaps and hash tables.
We have decided to forgo generality to keep the code clean and readable.

\section{General dictionary API}
All implemented data structures can be used through a~common API defined
in \texttt{dict/dict.h}. To encapsulate implementation details from
users of the API, we have used a~common C ``trick'', where we represent
dictionaries as ``virtual table pointers'' and ``private data''.
The common API represents a~dictionary as a~pointer to an opaque structure.
The type pointed to is declared in \texttt{dict/dict.h} as:
\begin{lstlisting}[language=C]
typedef struct dict_s dict;
\end{lstlisting}

The \texttt{struct dict\_s} structure is defined in \texttt{dict/dict.c}
to prevent exposure of implementation details from users including
\texttt{dict/dict.h}:
\begin{lstlisting}[language=C]
struct dict_s {
	void* opaque;
	const dict_api* api;
};
\end{lstlisting}

The \texttt{opaque} pointer is maintained by the concrete implementation.
\texttt{dict\_api} is a~``virtual method table'' type defined in
\texttt{dict/dict.h}:

\begin{lstlisting}[language=C]
typedef struct {
	int8_t (*init)(void**);
	void (*destroy)(void**);

	bool (*find)(void*, uint64_t key, uint64_t *value);
	bool (*insert)(void*, uint64_t key, uint64_t value);
	bool (*delete)(void*, uint64_t key);

	const char* name;

	// More function pointers.
} dict_api;
\end{lstlisting}

The \texttt{init} and \texttt{destroy} functions are passed a~pointer to the
\texttt{opaque} pointer and they respectively initialize and deinitialize it.
The \texttt{name} field is a~human-readable name of the data structure.
Other fields are given the \texttt{opaque} pointer as the first argument
and they act as implementations of their respective general versions operating
on general dictionaries declared in \texttt{dict/dict.h}:

\begin{lstlisting}[language=C]
bool dict_find(dict*, uint64_t key, uint64_t *value);
bool dict_insert(dict*, uint64_t key, uint64_t value);
bool dict_delete(dict*, uint64_t key);
\end{lstlisting}

Each of the functions above returns \texttt{true} on success and \texttt{false}
on failure. In particular, \texttt{dict\_find} and \texttt{dict\_delete} fail
if the dictionary does not contain the \texttt{key} and \texttt{dict\_insert}
fails if the dictionary already contains the inserted \texttt{key}.

Bigger problems, such as internal inconsistencies or failures to allocate
memory are signalled by logging a message and exitting the program.
While such resignation to error handling is not appropriate in dependable
software, our code is indended only for experimental purposes, where fatal
failure is acceptable. A more robust implementation would instead react
to failure by deallocating resources, undoing performed changes and returning
an error code.

Dictionary data structures may optionally implement successor and predecessor
lookups. The public API consists of the functions \texttt{dict\_next} and
\texttt{dict\_prev}. If the data structure does not implement these operations,
it may set the appropriate fields in \texttt{dict\_api} to \texttt{NULL}.
The \texttt{dict\_api\_allows\_order\_queries} and
\texttt{dict\_allows\_order\_queries} functions detect whether ordered
dictionary operations are available for the given implementation or dictionary.
\begin{lstlisting}[language=C]
bool dict_api_allows_order_queries(const dict_api*);
bool dict_allows_order_queries(const dict*);
bool dict_next(dict*, uint64_t key, uint64_t *next_key);
bool dict_prev(dict*, uint64_t key, uint64_t *prev_key);
\end{lstlisting}
If we iterate over a~dictionary in key order, many data structures can benefit
from keeping a~``finger'' on the last returned key-value pair. For example,
if we enumerate the keys of a~\mbox{B-tree} in order without auxilliary data,
we need to perform $\O(\log N)$ operations on every query. This can be easily
improved to $\O(1)$ amortized by stashing the last visited node between
queries. For simplicity, we did not implement order queries with a~finger.
While this makes our data structures slower on key enumerations, we believe
the penalty is roughly similar on all data structures, so our results should
still be representative enough.

Every data structure implementation exposes a~global variable of type
\texttt{const dict\_api} named \texttt{dict\_mydatastructure}. For example,
a B-tree can be used as follows:

\begin{lstlisting}[language=C]
#include <inttypes.h>
#include <stdio.h>

#include "dict/btree.h"
#include "dict/dict.h"

void example_btree(void) {
	dict* btree;
	dict_init(&btree, &dict_btree);
	if (!dict_insert(btree, 1, 100)) {
		printf("Error inserting 1 -> 100.\n");
	}
	// ...
	const bool found = dict_find(btree, 42, &value);
	if (found) {
		printf("Found 42 -> \%" PRIu64 ".\n",
				value);
	} else {
		printf("No value for 42.\n");
	}
	// ...
	if (!dict_delete(btree, 1)) {
		printf("Failed to remove key 1.\n");
	}
	dict_destroy(&btree);
}
\end{lstlisting}

This approach requires an indirect jump through the ``virtual method table''
whenever we write implementation-agnostic code. Removing this indirect jump
by directly compiling against a~particular implementation is likely to slightly
speed up the code. On the other hand, our tests are usually long-running, so in
the typical case, the target of the indirect jump will be cached, which should
imply a~very low overhead. Even if this indirect jumping were expensive, it
would not significantly affect our results -- every data structure under test
is invoked through indirect jumping, so the performance penalty is the same
on every data structure. While absolute data structure speed may be impacted
by this design decision, relative differences between data structures should
stay the same with a~virtual call or without.

Finally, we have decided to reserve one key for internal purposes.
This key is defined in \texttt{dict/dict.h} as the macro
\texttt{DICT\_RESERVED\_KEY} and its value is \texttt{UINT64\_MAX}.
Inserting this key into a~dictionary or deleting it will result in an~error.
Reserving this value is useful for representing unused slots in nodes of
\mbox{B-trees} or $k$-splay trees.

We implemented the following dictionary data structures:
\begin{itemize}
\item \texttt{dict\_array}, declared in \texttt{dict/array.h}:
	a sorted array of key-value pairs with $\O(N)$ insertions and deletions,
	indended as a~benchmark on very small inputs.
\item \texttt{dict\_btree}, declared in \texttt{dict/btree.h}:
	a 2-3-4 tree with nodes aligned to $64\,\text{B}$ cache lines.
	This is a~slight relaxation of B-trees (Chapter~\ref{chapter:btree})
	that allows inner nodes to contain $k\in[1;3]$ keys.
	An inner node needs to keep $k$~keys and $k+1$~pointers, so, with
	64-bit key and pointers and 64~B cache lines, an inner node fitting
	into a~cache line can keep at most 3 keys. However, leaves hold $k$
	keys and $k$ values of 64 bits each, which lets us put up to
	4 key-value pairs into each leaf.
\item \texttt{dict\_cobt}, declared in \texttt{dict/cobt.h}:
	a cache-oblivious B-tree (Chapter~\ref{chapter:cob}).
\item \texttt{dict\_htcuckoo}, declared in \texttt{dict/htcuckoo.h}:
	cuckoo hash table with simple tabulation hashing
	(Section~\ref{sec:cuckoo}).
\item \texttt{dict\_htlp}, declared in \texttt{dict/htlp.h}:
	an open-addressing hash table with linear probing and simple tabulation
	hashing (Section~\ref{sec:open-addressing}).
\item \texttt{dict\_splay}, declared in \texttt{dict/splay.h}:
	a splay tree (Chapter~\ref{chapter:splay}).
\item \texttt{dict\_ksplay}, declared in \texttt{dict/ksplay.h}:
	a $k$-splay tree (Chapter~\ref{chapter:ksplay}). % FIXME: which K?
% TODO: htable
\end{itemize}

\begin{figure}
\centering
\includegraphics[width=\linewidth]{img/btree}
\caption{A 2-3-4 tree with bigger leaves (\texttt{dict\_btree})}
\end{figure}

\section{Performance measurement}

% TODO: measure memory
Our goal is to have fast data structures with reasonable memory usage.
We measure the performance of the various dictionary implementations
on several experimental setups.

We tracked the time it took to conduct each experiment using the POSIX function
\texttt{clock\_gettime}. To better understand the bottlenecks, we also
tracked certain hardware events of interest using the \texttt{perf\_events} API
of the Linux kernel. This API is a~generic wrapper around platform-specific
performance counters. Events such as cache misses or branch mispredictions
increment these counters.
Details about performance monitoring on Intel CPUs can be found in
chapters 18 and 19 of the Intel 64 and IA-32 Architectures Software
Developer’s Manual\cite{intel-manual}.
% TODO: track RAM
% TODO: cite perf

\section{Synthetic experiments}
Each synthetic experiment setup has a tunable \emph{size} $S$
(i.e.,\ the maximum number of stored key-value pairs needed by the experiment).
The synthetic setups include:
\begin{itemize}
\item
	\emph{Random inserts and finds}: We generate pseudorandom key-value
	pairs by applying two deterministic invertible functions $k(i)$
	and $v(i)$ for $i$ between 0 and $S-1$.
	These simple functions use a Feistel network\footnote{%
		A Feistel network is a~simple construction for building
		symmetric ciphers from one-way functions. A~treatment of
		several ciphers based on Feistel networks is included in
		\cite{applied-cryptography}.
	} to provide
	``sufficiently randomly behaving'' key-value pairs while letting us
	derive $i$ from $k(i)$ or $v(i)$, which aids debugging.

	After the dictionary is seeded, we read items from it in random order.
	Because the difference between speeds of successful and unsuccessful
	\textsc{Find}s is significant on some dictionaries, a fraction of
	reads will request nonexistant keys. This fraction is set
	per-experiment.

	On this experiment, we separately measure the performance of insertions
	and finds.

\item
	\emph{Left-to-right scans}: We seed the structure with $S$ items
	as above. Then we traverse the dictionary from the minimum key to
	the maximum key, reading the values as we go. We don't include
	the seeding in our measurements.
	Left-to-right scans are only available on implementations that
	allow predecessor and successor queries.

\item
	\emph{Working set patterns}: The working set experiment needs
	a fixed integer parameter $W<S$. We again randomly seed the dictionary
	with $S$ items. Finally, $S$ times we access the key-value pair
	with key $k(i)$, where $i$ is picked pseudorandomly from
	$\{0,\ldots,W-1\}$.
	This access pattern is intended to simulate datasets, which have
	a mostly-static non-uniform access probability distribution.

\item
	\emph{Word occurrence counting}: We load a large text document
	and we tokenize it into words. Each word is then normalized
	to lowercase and transformed into a 64-bit integer via a~simple
	hash function $h$.\footnote{%
		We use the 64-bit version of the FNV-1 hash function
		(\url{http://www.isthe.com/chongo/tech/comp/fnv/}).
	} We create a dictionary storing counts of word
	occurrences keyed by word hashes. Each word $w$ is inserted into
	the dictionary by either inserting a new $\langle h(w), 1\rangle$ pair,
	or deleting an existing pair for $h(w)$ and inserting a new one
	with an incremented value.
	This usage pattern approximates building a search index on the document.
	\footnote{%
		We used \emph{The Complete Works of William Shakespeare}
		from Project Gutenberg
		(\url{http://www.gutenberg.org/ebooks/100}).
		% FIXME: cite this?
	}
\end{itemize}
The code of synthetic experiments is located in \texttt{experiments/performance}.

\section{Non-synthetic experiments}

A survey of the performance of different binary search tree implementations
on concrete real-life workloads was presented in \cite{libavl}.
In particular, splay trees were reported to be several times faster than AVL or
red-black trees when maintaining virtual memory areas of the Mozilla browser,
VMware or the Squid HTTP proxy. The author also simulated the workload of
the mapping from peer IP addresses to IP datagram identification nonces.
On this workload, splay trees underperformed balanced search trees.

To investigate the practical performance of our data structure implementations,
we decided to collect recordings of dictionary access patterns in real programs
and to collect performance metrics on replays of these recordings.

\subsection{Mozilla Firefox}
The Mozilla Firefox web browser contains an unordered dictionary template class
named \texttt{nsTHashtable} in \texttt{xpcom/glue/nsTHashtable.h}.
The default implementation uses double hashing. The template class implements
the \textsc{Find}, \textsc{Insert} and \textsc{Delete} operations. Additionally,
it supports enumerating all key-value pairs in the structure in an arbitrary
order.

We placed simple logging into the implementations of \textsc{Find},
\textsc{Insert} and \textsc{Delete}. For simplicity, we did not log
enumerations. Except splay trees, every data structure we implemented
can be extended to implement arbitrary-order enumerations in $\O(N/B)$ block
transfers, so we believe the effect of enumerations on the relative performance
differences between data structures would be relatively small.

After a browsing session, we counted the number of operations logged
on every \texttt{nsTHashtable} instance and we selected the top 10
instances with most operations.

Unlike our framework, \texttt{nsTHashtable} allows arbitrary-size keys and
values. We did not record stored values, as this experiment was only indended to
gather practical access patterns, which depend only on keys.
For hash tables with keys smaller than 64 bits, we simply zero-padded the keys
for our data structures. Bigger keys were compressed by applying the FNV-1
hash function to individual octets.

Presumably for the sake of optimization, \texttt{nsTHashtable} has a more
liberal interface than we implemented. Aside from searches and updates,
\texttt{nsTHashtable} also supports enumerating all key-value pairs in
an arbitrary order (i.e.,\ depending on the internal hash function).
The user is also allowed to update values in enumerated key-value pairs
at no extra \textsc{Find} cost.
We skipped arbitrary enumeration when replaying recorded operations.
Because further calls to \texttt{nsTHashtable} may depend on the arbitrary
order of enumeration, we expect that \texttt{nsTHashtable} would behave
better on recorded operations than any of our structures. For example,
since \texttt{nsTHashtable} will enumerate keys in the order in which they
are stored within \texttt{nsTHashtable}, \texttt{nsTHashtable} will reap
the rewards of cache friendliness, while our data structures will be much
more likely to read from unpredictable locations.

Fairly comparing data structures in the presence of arbitrary-order enumeration
would require actually running our source code within the instrumented program.
We did not pursue this direction further in this thesis, but benchmarking data
structures directly where they are used may provide inspiration for better
optimizations. Particularly interpreters of dynamic programming languages
with built-in dictionary types, like Python or Ruby, could prove interesting
test subjects.

Our solution to recording the operations was very simple: we applied a small
patch to \texttt{nsTHashtable} that printed out operations to standard output,
where we collected them and grouped them by the \texttt{this} pointer.
To further simplify the patch, we made no attempt to synchronize output,
and we processed the output by a simple script that removed any lines that
could not be successfully parsed. The fraction of removed unparseable lines
was approximately 23\%.

\subsection{Geospatial database}
To test the performance of \textsc{FindNext} and \textsc{FindPrevious}, we
simulated the workload of a simple database of geospatial data supporting
local searching. The source code for this experiment is stored in
\texttt{experiments/cloud}. It can be built by running \texttt{make
bin/experiments/cloud}. Before running the experiment, input data must
be downloaded by running \texttt{./download.sh} from within
\texttt{experiments/cloud}.

We used data from \emph{Extended Edited Synoptic Cloud Reports from Ships and
Land Stations Over the Globe, 1952-2009 (NDP-026C)} \cite{cloud-reports}.
The main part of this dataset consists of 196 compressed plaintext files
containing one cloud report per line. Aside from many other fields,
each report contains the date and UTC hour, the longitude and latitude of the
reporting ship or land station and the measured sea level pressure and
wind speed.
Our simulated database loads all air temperature and wind speed measurements
and indexes them by their coordinates. Given a position on the globe, the
database can return a set of reports taken close to the position.

To implement this database in the context of our general dictionary API,
we map from latitude-longitude pairs to dictionary keys via a
locality-preserving mapping called the \emph{Z-order} (or
\emph{Morton order} after its inventor, who introduced it in
\cite{morton-order}). The Z-order computes a one-dimensional key from
a position in multiple dimensions simply by interleaving the bits of
the coordinates. One-dimensional distances implied by the Z-order only
approximate the original two-dimensional distances, so the Z-order is
not useful for exact closest-point queries -- we only use it as an
approximation. For the sake of simplicity, we also interpret
latitude-longitude angles as plane coordinates, without any attempt
to merge 180\textdegree~W and 180\textdegree~E.

Because stations at one position usually give multiple reports,
the key for our dictionary also needs to contain a unique identifier of
the report, which we create by counting the reports from each unique position
and append after the Z-order code of the position.
Some stations also do not provide the wind speed or air pressure.
Records from stations which provide neither are not loaded, and records
with missing values are also skipped at query time.

\begin{figure}
\centering
\begin{tikzpicture}
\node at (0,0) (z_2x2) {\begin{tikzpicture}
	\draw[step=1cm,gray,very thin] (0,0) grid (2,2);
	% NOTE: TikZ coordinate system has (0,0) in bottom left.
	\draw[thick] (0.5,1.5) -- (1.5,1.5) -- (0.5,0.5) -- (1.5,0.5);
	\end{tikzpicture}
};
\node at (4,0) (z_4x4) {\begin{tikzpicture}
	\draw[step=1cm,gray,very thin] (0,0) grid (4,4);
	% NOTE: TikZ coordinate system has (0,0) in bottom left.
	\draw[thick] (0.5,3.5) -- (1.5,3.5) -- (0.5,2.5) -- (1.5,2.5) --
		(2.5,3.5) -- (3.5,3.5) -- (2.5,2.5) -- (3.5,2.5) --
		(0.5,1.5) -- (1.5,1.5) -- (0.5,0.5) -- (1.5,0.5) --
		(2.5,1.5) -- (3.5,1.5) -- (2.5,0.5) -- (3.5,0.5);
	\end{tikzpicture}
};
\end{tikzpicture}
\caption{Z-order in two dimensions}
\label{fig:z-order}
\end{figure}

A possible alternative would be using a \emph{Peano curve}
(introduced in \cite{peano-curve}, later generalized into
\emph{Hilbert curves} of arbitrary dimension by \cite{hilbert-curve}).
A desirable property of Peano curves is that if two-dimensional points
$x$ and $y$ map to neighbouring images in the one-dimensional space, then
$x$ and $y$ were neighbours in the two-dimensional space. As apparent
from Figure \ref{fig:z-order}, this does not hold in the Z-order.
We decided to use the Z-order for the simplicity of its mapping function.

Queries are implemented simply by mixed calls to \texttt{dict\_prev} and
\texttt{dict\_next} until we collect enough results. A non-toy database might
perform some additional filtering to ensure only reports within a certain
distance are returned.

Our usage of this simulated database will pick $S$ random points on the globe.
For each point, we fetch $R$ close records and we calculate the average
recorded sea-level atmospheric pressure and wind speed in these records.
Atmospheric pressures and wind speeds are packed into 64-bit values, which
are stored in the ordered dictionary.

\begin{figure}
\centering
\begin{subfigure}[b]{1.0\textwidth}
	\includegraphics[width=\textwidth]{img/cloud/stations}
	\caption{Stations providing wind speed or air pressure data}
\end{subfigure}

\begin{subfigure}[b]{1.0\textwidth}
	\includegraphics[width=\textwidth]{img/cloud/wind-speed}
	\caption{Average wind speeds}
\end{subfigure}
\end{figure}
\clearpage
\begin{figure}[H]
\centering
\ContinuedFloat
\begin{subfigure}[b]{1.0\textwidth}
	\includegraphics[width=\textwidth]{img/cloud/pressure}
	\caption{Average sea-level atmospheric pressure}
\end{subfigure}
\caption{Results from the cloud data experiment.
	Each query sampled 1000 close measurements.}
\label{fig:cloud-data}
\end{figure}
