\chapter{Models of memory hierarchy}
\label{chapter:models}
\section{RAM}

The performance of algorithms is usually measured in the RAM
(\emph{Random Access Memory} or \emph{Machine}) model.
The ideal RAM machine has an infinite memory divided into cells addressed
by integers.
One instruction of the RAM machine takes constant time and it
can compute an arithmetic expression based on values of cells and store it
in another cell. The expressions may involve reading cells, including cells
pointed to by another cell. The exact set of arithmetic operations allowed
on the values differs from model to model, but a typical set would include
the usual C operators (\texttt{+}, \texttt{-}, \texttt{*}, \texttt{/},
\texttt{\%}, \texttt{\&}, \texttt{|}, \texttt{\^}, \texttt{~}, \dots).
A complete set of instructions also requires a conditional jump (based on
the value of an expression).
A more thorough treatment of various types of RAM is included
in \cite{saga-of-msts}.

The model is also restricted by the \textit{word size} (denoted $w$),
which is the number of bits allocated for a cell. Choosing a too large $w$
results in an unreasonably strong model. A typical choice of $w$ is
$\Theta(\log N)$, where $N$ is size of input.
A RAM with this word size is called the \textit{transdichotomous model}
by virtue of unifying the notion of problem size and word size.

We benchmark RAM algorithms by their \emph{time} and \emph{space complexity}.
The time complexity is the number of executed instructions as a function of
input size and the space complexity is the maximum cell index accessed during
execution\footnote{
	The number of overwritten cells would be a simpler definition
	of space complexity, but that would allow unrealistic ``dirty tricks''
	like maintaining an unordered set with deterministic $\O(1)$ time
	for both updates and lookups by interpreting the key as a memory
	location.
}.

The time and space used by real computer programs is obviously related
to the predictions made by the RAM model. However, the RAM model is far
from equivalent with real computers, especially so when we are dealing with
very large inputs. One significant source of this discrepancy is the assumption
that all memory accesses cost the same $\O(1)$ time. In today's computers,
the difference between accessing 1 kB cached in L1 cache and accessing
another 1 kB stored on swapped memory is so large that it cannot be ignored.

\section{Memory hierarchies}
% TODO: latency numbers everyone should know
% TODO: list-array experiment
There are two major types of RAM: \emph{static} and \emph{dynamic}.
While static RAM stores information as active states of semiconductors, dynamic
RAM uses capacitors, which lose charge over time. To avoid losing information,
contents of dynamic RAM must be periodically refreshed by ``reading'' the data
and writing it back. Dynamic RAM also has destructive reads: a read removes
charge from capacitors, which must then be returned to keep the data.
In general, static RAM is fast and expensive, while dynamic RAM is slow,
cheap and more space-efficient. Main memories are usually dynamic.

CPU speeds are growing faster (60\% per year) than speeds of memory (10\% per
year) \cite{Ailamaki:2004:DAN:1316689.1316801}.
A fast CPU needs to wait several hundred cycles to perform a RAM access.
TODO: source "several hundred"
To alleviate the costs of memory transfers, several levels of static RAM
caches are inserted between the CPU and the main memory. The caches are
progressively smaller and faster the closer they are to the CPU. The difference
between speeds of memory levels reaches several orders of magnitude. TODO: source

Larger and slower memories are also faster when accessing data physically
close to recently accessed data. The hard disk is the most apparent example
of this behavior -- reading a byte close after the current position of the drive
head takes very little time compared to reading a random position, which incurs
a slow disk seek. % TODO: times
Similarly, modern CPUs contain a prefetcher, which tries to speculatively
retrieve data from main memory before it is actually accessed.

Furthermore, if we need to wait for a long time to get our data from a slow
store, it is wise to ask for a lot of data at once. Imagine an HTTP connection
moving a 10 MB file across the Atlantic: the time to compose and send the HTTP
request and to parse a response dwarfs the latency of the connection.
Memory hierarchies transport data between memories in \emph{blocks}.
In the context of caches and main memory, these blocks are called
\emph{cache lines} and their usual size is 64 B.
Operating systems usually perform disk I/O in batches of 4 kB,
while physical HDD blocks typically span either 512 B, or 4 kB.

\begin{figure}
\begin{itemize}
\item Processor registers. Their latency is usually one CPU cycle.
	The installed Intel Core i5-3230M CPU has 16 logical 64-bit
	general-purpose registers (plus some registers for specific purposes,
	e.g.\ vector operations).
\item Level 1 data and instruction caches (L1d, L1i), both 8-way associative
	and 32 kB in size. Their latency is several CPU cycles.
	% There are two L1 caches, one for each CPU core.
\item Level 2 (L2) cache, 8-way associative, 256 kB in size.
	It is an order of magnitude slower than the L1 cache. Again, each
	CPU core has a separate L2 cache.
\item Level 3 (L3) cache, 12-way associative, 3 MB in size.
	The latency is higher than that of the L2 cache. Shared between CPU
	cores.
\item Main memory, 6 GB.
\item Disks: a 120 GB SSD and a 500 GB HDD.
\end{itemize}
\caption{The memory hierarchy of a Lenovo X230 laptop}
\end{figure}

While programs may explicitly control transfers between the disk and main
memory, the L1, L2 and L3 caches are under control of hardware.
It is possible to advise the processor via special instructions about some
characteristics of memory access (e.g.\ by requesting prefetching), but programs
cannot explicitly set which memory blocks should be stored in caches.

\emph{Fully associative caches} maintain up to $m$ memory blocks called
\textit{cache lines} (typically of 64 B). When the CPU tries to access
a memory location, the cache is checked first before accessing main memory.
If the cache doesn't contain the requested data, it is loaded into the cache.
When the cache becomes full, it needs to evict presently loaded cache lines
before caching new ones. The evicted cache line is picked according
to a \emph{cache replacement policy}. The most commonly implemented cache
replacement policy is LRU (\emph{least recently used}), which evicts
the memory line that was accessed least recently.
A theoretically useful, but practically impossible policy is the \emph{optimal
replacement policy}, which always evicts the cache line that will be needed
farthest in the future.

Because fully associative caching would be hard to implement in hardware,
actual caches have a limited associativity. While fully associative
caches may cache an arbitrary set of memory lines, $t$-way associative
caches only allow each memory location to reside in one of $t$ possible
locations in the cache. If all $t$ possible locations for a memory line
are already occupied, the cache replacement policy evicts one of the $t$
lines to make space fresh data.

The associativity of caches $t$ is usually a small power of two, like 8 or 16.
The set of possible cache locations for lines in memory is determined
by simply masking out a few address bits. This approach implies that
accessing too many locations with addresses differing exactly by a power
of two may result in \emph{cache aliasing} -- the cache will only be able
to store only $t$ useful cache lines at once, which greatly degrades its
performance.

Programs which have \emph{spatial} and \emph{temporal locality of reference}
will perform better on computers with hierarchical memory. Several models have
been developed to benchmark algorithms designed for hierarchical memory.

\section{The external memory model}
The simplest model of memory hierarchies has only two levels of memory:
the \emph{cache} and the \emph{disk}\footnote{
	We choose this naming to avoid the ambiguous term ``memory'',
	which may mean both ``main memory'' in relation to a disk,
	or an ``external memory'' in relation to a cache.
}. The motivation for omitting all other levels is that for practical purposes,
only one gap between memory levels is usually the bottleneck. For example,
the bottleneck in a 10 TB database would probably be disk I/O, because
only a negligible part of the database would fit into main memory and
lower cache levels. If the time to make a query is 30 ms, optimizing transfers
between L2 cache and RAM would be unlikely to help.

The cache and the disk are divided to blocks of size $B$. The size of the cache 
in bytes is fixed and is denoted $M$.
In the external memory model, we allow free computation on data
stored in the cache. The time complexity is measured in the number of
\emph{block transfers}, which read or write an entire block to the disk.
Algorithms are allowed to explicitly transfer blocks, which corresponds
to the boundary between main memory and disks in computers.
The space complexity is the index of the last overwritten block.
The parameters $B$ and $M$ are known to the program.

\cite{em-ads} is a comprehensive survey of the external memory model
(extended to handle an array of disks allowing simultaneous access), including
optimal sorting algorithms (in $\Theta(\frac{N}{B}\log_{M/B}\frac{N}{B})$
block transfers), algorithms for computational geometry, graph algorithms
and dictionary data structures.

\section{Cache-oblivious algorithms}
Cache-oblivious algorithms run on a machine with a two-level memory hierarchy.
Unlike algorithms in the external memory model, cache-oblivious algorithms
are not allowed access to $M$ and $B$. They may only assume the existence
of a main memory and a cache.
The algorithm must perform well independent of the values of $M$ and $B$.
On a machine with multiple levels of memory hierarchy, this means that
the algorithm performs well on every cache boundary at the same time.
Thanks to this property, cache-oblivious algorithms need little tuning to the
computer they run on. Interestingly, automatically optimized cache boundaries
include not only the usual L1, L2 and L3 caches, but also the \emph{translation
lookaside buffer} (TLB), which caches the mapping of pages from virtual to
physical memory.

The cache is assumed to be tall (with $\Omega(B)$ blocks) and
the algorithm cannot control block transfers between the cache and the memory.
This is similar to real computers: L1, L2 and L3 caches are also out
of the program's control.
The cache is, however, assumed to be unrealistically powerful: it is fully
associative and the replacement policy is optimal. Fortunately, fully
associative caches with optimal replacement can be simulated with a
$2\times$ slowdown on $2\times$ larger LRU or FIFO caches
\cite{sleator1985amortized}.
The performance of many cache-oblivious algorithms is only slowed down
by a constant factor when we halve the number of blocks, so cache-oblivious
algorithms can usually be adopted in real world programs with only a constant
slowdown compared to the model.

Many results from the external memory model have been repeated in the
cache-oblivious model, including optimal sorting in
$\Theta(\frac{N}{B}\log_{M/B}\frac{N}{B})$ block transfers and permuting
in $\Theta(\min\{N,\frac{N}{B}\log_{M/B}\frac{N}{B}\})$ block transfers.
On the other hand, the hiding of cache parameters in the cache-oblivious model
does slightly reduce its strength. \cite{limits-of-co} showed that no
cache-oblivious algorithm can archieve
$\Theta(\frac{N}{B}\log_{M/B}\frac{N}{B})$ block transfers for sorting
without the tall cache assumption. Cache-oblivious algorithms can also permute
either in $\Theta(N)$ or in $\Theta(\frac{N}{B}\log_{M/B}\frac{N}{B})$ block
transfers, but it is impossible for a cache-oblivious algorithm to pick
the faster option.
In chapter \ref{chapter:cob}, we show a data structure that is a cache-oblivious
equivalent of B-trees.
