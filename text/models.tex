\chapter{Models of memory hierarchy}
\label{chapter:models}
\section{RAM}

The performance of algorithms is usually measured in the RAM
(\emph{Random Access Memory} or \emph{Machine}) model.
The ideal RAM machine has an infinite memory divided into constant-size
cells addressed by integers.
One instruction fo the RAM machine can compute an arithmetic expression
based on values of cells and store it into another cell. The expressions
may involve reading cells, including cells pointed to by another cell.
The exact set of arithmetic operations allowed on the values differs
from model to model, but a typical set would include the usual C operators
(\texttt{+}, \texttt{-}, \texttt{*}, \texttt{/}, \texttt{\%}, \texttt{\&},
 \texttt{|}, \texttt{\^}, \texttt{~}, \dots).
A complete set of instructions also requires a conditional jump (based on
the value of an expression).

We benchmark RAM algorithms by their \emph{time} and \emph{space complexity}.
The time complexity is the number of executed instructions as a function of
input size and the space complexity is the maximum cell index accessed during
execution\footnote{
	The number of overwritten cells would be a simpler definition
	of space complexity, but that would allow unrealistic ``dirty tricks''
	like maintaining an unordered set with deterministic $\O(1)$ time
	for both updates and lookups by interpreting the key as a memory
	location.
}.

The time and space used by real computer programs is obviously related
to the predictions made by the RAM model. However, the RAM model is far
from equivalent with real computers, especially so when we are dealing with
very large inputs. One significant source of this discrepancy is the assumption
that all memory accesses cost the same $\O(1)$ time. In today's computers,
the difference between accessing 1 kB cached in L1 cache and accessing
another 1 kB stored on swapped memory is so large that it cannot be ignored.

\section{Memory hierarchies}
% TODO: latency numbers everyone should know
% TODO: list-array experiment
There are two major types of RAM: \emph{static} and \emph{dynamic}.
While static RAM stores information as states of logic gates, dynamic RAM
uses capacitors, which lose charge over time. To avoid losing information,
contents of dynamic RAM must be periodically refreshed by ``reading'' the data
and writing it back. Dynamic RAM also has destructive reads: a read removes
charge from capacitors, which must then be returned to keep the data.
In general, static RAM is fast and expensive, while dynamic RAM is slow,
cheap and more space-efficient. Main memories are usually dynamic.

% TODO: jak mam citovat tady???
CPU speeds are growing faster (60\% per year) than speeds of memory (10\% per
year)\cite{Ailamaki:2004:DAN:1316689.1316801}. A fast CPU needs to wait several
hundred cycles to perform a RAM access. TODO: source "several hundred"
To alleviate the costs of memory transfers, several levels of static RAM
caches are inserted between the CPU and the main memory. The caches are
progressively smaller and faster the closer they are to the CPU. The difference
between speeds of memory levels reaches several orders of magnitude. TODO: source

Larger and slower memories are also faster when accessing data physically
close to recently accessed data. The hard disk is the most apparent example
of this behavior -- reading a byte close to the current position of the drive
head takes very little time compared to reading a random position, which incurs
a slow disk seek. % TODO: times
Similarly, modern CPUs contain a prefetcher, which tries to speculatively
retrieve data from main memory before it is actually accessed.

% TODO: note about B, 1 kB = 1024 B
Furthermore, if we need to wait for a long time to get our data from a slow
store, it is wise to ask for a lot of data at once. Imagine an HTTP connection
moving a 10 MB file across the Atlantic: the time to compose and send the HTTP
request and to parse a response dwarfs the latency of the connection.
Memory hierarchies transport data between memories in \emph{blocks}.
In the context of caches and main memory, these blocks are called
\emph{cache lines} and their usual size is 64 B.
A traditional block size for disk I/O is 4 kB.

\begin{figure}
\begin{itemize}
\item Processor registers. Their latency is usually one CPU cycle.
	The installed Intel Core i5-3230M CPU has 16 logical 64-bit
	general-purpose registers (plus some registers for specific purposes,
	e.g. vector operations).
\item Level 1 (L1) cache, 8-way associative. The latency is several CPU cycles.
	It is divided into a data section and an instruction section,
	both of them 32 kB in size. There are two L1 caches, one for each CPU
	core.
\item Level 2 (L2) cache, 8-way associative, 256 kB in size.
	It is an order of magnitude slower than the L1 cache. Again, each
	CPU core has a separate L2 cache.
\item Level 3 (L3) cache, 12-way associative, 3 MB in size.
	The latency is higher than that of the L2 cache. Shared between CPU
	cores.
\item Main memory, 6 GB.
\item Disks: a 120 GB SSD and a 500 GB HDD.
\end{itemize}
\caption{The memory hierarchy of a Lenovo X230 laptop}
\end{figure}

While programs may explicitly control transfers between the disk and main
memory, the L1, L2 and L3 caches are under control of hardware.
It is possible to advise the processor via special instructions about some
characteristics of memory access (e.g. by requesting prefetching), but programs
cannot explicitly set which memory blocks should be stored in caches.

Caches are usually $m$-way associative: for every line in main memory, there
are $m$ possible places where it can be stored in cache. If all $m$ possible
slots are already occupied by other lines, one of the loaded line gets evicted
to make space for newly requested data. The node that gets evicted is determined
according to the \emph{cache replacement policy}. The most common cache
replacement policy is LRU (\emph{least recently used}), which evicts
the memory line that was accessed least recently.

Programs which have \emph{spatial} and \emph{temporal locality of reference}
will perform better on computers with hierarchical memory. Several models have
been developed to benchmark algorithms designed for hierarchical memory.

\section{The external memory model}
The simplest model of memory hierarchies has only two levels of memory:
the \emph{cache} and the \emph{disk}\footnote{
	We choose this naming to avoid the ambiguous term ``memory'',
	which may mean both ``main memory'' in relation to a disk,
	or an ``external memory'' in relation to a cache.
}. The motivation for omitting all other levels is that for practical purposes,
only one gap between memory levels is usually the bottleneck. For example,
the bottleneck in a 10 TB database would probably be disk I/O, because
only a negligible part of the database would fit into main memory and
lower cache levels. If the time to make a query is 30 ms, optimizing transfers
between L2 cache and RAM would be unlikely to help.

The cache and the disk are divided to blocks of size $B$. The size of the cache 
in bytes is fixed and is denoted $M$.
In the external memory model, we allow free computation on data
stored in the cache. The time complexity is measured in the number of
\emph{block transfers}, which read or write an entire block to the disk.
Algorithms are allowed to explicitly transfer blocks, which corresponds
to the boundary between main memory and disks in computers.
The space complexity is the index of the last overwritten block.
The parameters $B$ and $M$ are known to the program.

TODO: writeup, cite source
TODO: $B<N$

\section{Cache-oblivious algorithms}
% TODO: cite frigo et al. 1999
Cache-oblivious algorithms run on a machine with a two-level memory hierarchy.
Unlike algorithms in the external memory model, cache-oblivious algorithms
are not allowed access to $M$ and $B$. They may only assume the existence
of a main memory and a cache.
The algorithm must perform well independent of the values of $M$ and $B$.
On a machine with multiple levels of memory hierarchy, this means that
the algorithm performs well on every cache boundary at the same time.
Thanks to this property, cache-oblivious algorithms need little tuning to the
computer they run on.

The cache is assumed to be tall (with number of blocks $>> B^2$) and
the algorithm cannot control block transfers between the cache and the memory.
This is similar to real computers: L1, L2 and L3 caches are also out
of the program's control.
Cache-oblivious algorithms assume a slightly unrealistic cache model:
the cache is fully associative (memory blocks can be stored anywhere in the
cache) and the replacement policy is optimal (the cache evicts the memory
block that will be used the farthest in the future).
Fortunately, fully associative caches with optimal replacement can be simulated
with a linear slowdown on less ideal caches, so results obtained in
the cache-oblivious model also apply to the real world.

TODO: basic bounds
