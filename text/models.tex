\chapter{Models of memory hierarchy}
\section{RAM}

The performance of algorithms is usually measured in the RAM
(\textit{Random Access Memory} or \textit{Machine}) model.
The ideal RAM machine has an infinite memory divided into constant-size
cells addressed by integers.
One instruction fo the RAM machine can compute an arithmetic expression
based on values of cells and store it into another cell. The expressions
may involve reading cells, including cells pointed to by another cell.
The exact set of arithmetic operations allowed on the values differs
from model to model, but a typical set would include the usual C operators
(\texttt{+}, \texttt{-}, \texttt{*}, \texttt{/}, \texttt{\%}, \texttt{\&},
 \texttt{|}, \texttt{\^}, \texttt{~}, \dots).
A complete set of instructions also requires a conditional jump (based on
the value of an expression).

We benchmark RAM algorithms by their \textit{time} and \textit{space complexity}.
The time complexity is the number of executed instructions as a function of
input size and the space complexity is the maximum cell index accessed during
execution\footnote{
	The number of overwritten cells would be a simpler definition
	of space complexity, but that would allow unrealistic "dirty tricks"
	like maintaining an unordered set with deterministic $\O(1)$ time
	for both updates and lookups by interpreting the key as a memory
	location.
}.

The time and space used by real computer programs is obviously related
to the predictions made by the RAM model. However, the RAM model is far
from equivalent with real computers, especially so when we are dealing with
very large inputs. One significant source of this discrepancy is the assumption
that all memory accesses cost the same $\O(1)$ time. In today's computers,
the difference between accessing 1 kB cached in L1 cache and accessing
another 1 kB stored on swapped memory is so large that it cannot be ignored.
% TODO: latency numbers everyone should know

% TODO: list-array experiment

There are two major types of RAM: \textit{static} and \textit{dynamic}.
While static RAM stores information as states of logic gates, dynamic RAM
uses capacitors, which lose charge over time. To avoid losing information,
contents of dynamic RAM must be periodically refreshed by "reading" the data
and writing it back. Dynamic RAM also has destructive reads: a read removes
charge from capacitors, which must then be returned to keep the data.
In general, static RAM is fast and expensive, while dynamic RAM is slow,
cheap and more space-efficient. Main memories are usually dynamic.

CPU speeds are growing faster than speeds of dynamic RAM. A fast CPU needs to
wait several hundred cycles to perform a RAM access. TODO: source "several hundred"
To alleviate the costs of memory transfers, several levels of static RAM
caches (usually at least L1 and L2) are inserted between the CPU and
the main memory. The caches are progressively smaller and faster the closer
they are to the CPU. The difference between speeds of memory levels reaches
several orders of magnitude. TODO: source

Larger and slower memories are also faster when accessing data physically
close to recently accessed data. The hard disk is the most apparent example
of this behavior -- reading a byte close to the current position of the drive
head takes very little time compared to reading a random position, which incurs
a slow disk seek. % TODO: times
Similarly, modern CPUs contain a prefetcher, which tries to speculatively
retrieve data from main memory before it is actually accessed.

% TODO: note about B, 1 kB = 1024 B
Furthermore, if we need to wait for a long time to get our data from a slow
store, it is wise to ask for a lot of data at once. Imagine an HTTP connection
moving a 10 MB file across the Atlantic: the time to compose and send the HTTP
request and to parse a response dwarfs the latency of the connection.
Memory hierarchies transport data between memories in \textit{blocks}.
In the context of L1 and L2 caches and main memory, these blocks are called
\textit{cache lines} and their usual size is 64 B.
A traditional block size for disk I/O is 4 kB.

Programs which have \textit{spatial} and \textit{temporal locality of reference}
will perform better on computers with hierarchical memory. Several models have
been developed to benchmark algorithms designed for hierarchical memory.

\section{The external memory model}
The simplest model of memory hierarchies has only two levels of memory:
the \textit{cache} and the \textit{disk}\footnote{
	We choose this naming to avoid the ambiguous term "memory",
	which may mean both "main memory" in relation to a disk,
	or an "external memory" in relation to a cache.
}. The motivation for omitting all other levels is that for practical purposes,
only one gap between memory levels is usually the bottleneck. For example,
the bottleneck in a 10 TB database would probably be disk I/O. If the time
to make a query is 10 ms, optimizing transfers between L2 cache and RAM would
be unlikely to help.

The cache and the disk are divided to blocks of size $B$. The size of the cache 
in bytes is fixed and is denoted $M$.
In the external memory model, we allow free RAM computation on the cache.
The time complexity is measured in the number of \textit{block transfers},
which read or write an entire block to the disk. The space complexity is
the index of the last overwritten block.
The parameters $B$ and $M$ are known to the program.

TODO: writeup, cite source
TODO: $B<N$

\section{Cache-oblivious algorithms}
Cache-oblivious algorithms run on a machine with external memory, but they are
not allowed access to $M$ and $B$. The algorithm must perform well on all
possible values of $M$ and $B$.
On a machine with multiple cache levels, this means that the algorithm performs
well on every cache boundary at the same time.

TODO: basic bounds

TODO: tall cache assumption?
