\chapter{Models of memory hierarchy}
\label{chapter:models}
\section{RAM}

The performance of algorithms is usually measured in the RAM
(\emph{Random Access Machine}\footnote{%
	Another meaning of the abbreviation ``RAM'' is \emph{Random Access
	Memory}. To avoid confusing the model and the type of computer memory,
	we will only use ``RAM'' to refer to the model.
}or \emph{Machine}) model.
The ideal RAM machine has an infinite memory divided into cells addressed
by integers.
One instruction of the RAM machine takes constant time and it
can compute a fixed arithmetic expression based on values of cells and store it
in another cell. The expressions may involve reading cells, including cells
pointed to by another cell. The exact set of arithmetic operations allowed
on the values differs from model to model, but a~typical set would include
the usual C operators (\texttt{+}, \texttt{-}, \texttt{*}, \texttt{/},
\texttt{\%}, \texttt{\&}, \texttt{|}, \texttt{\^},
\texttt{\raisebox{0.5ex}{\texttildelow}},
\dots).  A complete set of instructions also requires a~conditional jump (based
on the value of an~expression).
A more thorough treatment of various types of RAM is included
in \cite{saga-of-msts}.

The model is also restricted by the \textit{word size} (denoted~$w$),
which is the number of bits allocated for a~cell. Choosing a~too large $w$
results in an unreasonably strong model. A~typical choice of~$w$ is
$\Theta(\log N)$, where $N$ is size of input.
Since this choice of $w$ ties together the problem size and the word size,
a~RAM model with this word size is called the \textit{transdichotomous model}.

RAM algorithms are judged by their \emph{time} and \emph{space complexity}.
The time complexity is the number of executed instructions as a~function of
input size and the space complexity is the maximum cell index accessed during
execution.\footnote{%
	The number of overwritten cells would be a~simpler definition
	of space complexity, but that would allow unrealistic ``dirty tricks''
	like maintaining an unordered set with deterministic $\O(1)$ time
	for both updates and lookups by interpreting the key as a~memory
	location.
}

The time and space used by real computer programs is obviously related
to the predictions made by the RAM model. However, the RAM model is far
from equivalent with real computers, especially so when we are dealing with
very large inputs. One significant source of this discrepancy is the assumption
that all memory accesses cost the same $\O(1)$ time. In today's computers,
the difference between accessing $1\,\text{kB}$ cached in L1~cache and
accessing another $1\,\text{kB}$ in virtual memory paged-out to a~disk is so
large that it cannot be ignored.

\section{Memory hierarchies}
% TODO: latency numbers everyone should know
% TODO: list-array experiment
There are two major types of memory chips: \emph{static} and \emph{dynamic}.
While static memory stores information as active states of semiconductors,
dynamic memory uses capacitors, which lose charge over time. To avoid losing
information, contents of dynamic memory must be periodically refreshed by
``reading'' the data and writing it back. Dynamic memory also has destructive
reads: a~read removes charge from capacitors, which must then be re-charged
to keep the data. In general, static memory is fast and expensive, while dynamic
memory is slow, cheap and more space-efficient. Main memories are usually
dynamic.

CPU speeds are growing faster (60\% per year) than speeds of memory (10\% per
year) \cite{Ailamaki:2004:DAN:1316689.1316801}.
A fast CPU needs to wait several hundred cycles to access main memory.
For example, Intel Core i7 and Xeon 5500 processors with clocking frequencies
in GHz need approximately $100\,\text{ns}$ for a~memory access
access \cite{perf-analysis-guide}.
To alleviate the costs of memory transfers, several levels of static memory
caches are inserted between the CPU and the main memory. The caches are
progressively smaller and faster the closer they are to the CPU. The difference
between speeds of memory levels reaches several orders of magnitude.

Larger and slower memories are also faster when accessing data physically
close to recently accessed places. The hard disk is the most apparent example
of this behavior -- reading a~byte at the current position of the drive
head takes very little time compared to reading a~random position, which incurs
a slow disk seek. % TODO: times
Similarly, modern CPUs contain a~prefetcher, which tries to speculatively
retrieve data from main memory before it is actually accessed.
Modern memory is also structured as a matrix, and reading a location consists
of first reading its row, and then extracting the column. Reading from the last
accessed row is faster, since the row can be kept in a~prefetch buffer.

Furthermore, if we need to wait for a~long time to get our data from a~slow
store, it is wise to ask for a~lot of data at once. Imagine an HTTP connection
moving a~$10\,\text{MB}$ file across the Atlantic: the time to compose and send
the HTTP request and to parse a~response dwarfs the latency of the connection.
Memory hierarchies transport data between memories in \emph{blocks}.
In the context of caches and main memory, these blocks are called
\emph{cache lines} and their usual size is $64\,\text{B}$.
Operating systems usually perform disk I/O in batches of $4\,\text{kB}$,
while physical HDD blocks typically span either $512\,\text{B}$,
or $4\,\text{kB}$.

\begin{figure}
\begin{itemize}
\item Processor registers. Their latency is usually one CPU cycle.
	The installed Intel Core i5-3230M CPU has 16 logical 64-bit
	general-purpose registers (plus some registers for specific purposes,
	e.g.,\ vector operations).
\item Level 1 data and instruction caches (L1d, L1i), both 8-way associative
	and $32\,\text{kB}$ in size. Their latency is several CPU cycles.
	% There are two L1 caches, one for each CPU core.
\item Level 2 (L2) cache, 8-way associative, $256\,\text{kB}$ in size.
	It is an order of magnitude slower than the L1 cache. Again, each
	CPU core has a separate L2 cache.
\item Level 3 (L3) cache, 12-way associative, $3\,\text{MB}$ in size.
	The latency is higher than that of the L2 cache. Shared between CPU
	cores.
\item Main memory, $6\,\text{GB}$.
\item Disks: a $120\,\text{GB}$ SSD and a $500\,\text{GB}$ HDD.
\end{itemize}
\caption{The memory hierarchy of a Lenovo ThinkPad X230 laptop}
\end{figure}

While programs may explicitly control transfers between the disk and main
memory, the L1, L2 and L3 caches are under control of hardware.
It is possible to advise the processor via special instructions about some
characteristics of memory access (e.g.,\ by requesting prefetching), but
programs cannot explicitly set which memory blocks should be stored in caches.

A cache maintains up to $m$ memory blocks called \textit{cache lines},
typically of $64\,\text{B}$.
We will first describe \emph{fully associative caches}.
When the CPU tries to access a~memory location, the cache is checked first
before accessing main memory. If the cache doesn't contain the requested data,
it is loaded into the cache. When the cache becomes full (i.e.,\ when all $m$
cache lines are used), it needs to evict presently loaded cache lines before
caching new ones. The evicted cache line is picked according to a \emph{cache
replacement policy}. The most commonly implemented cache replacement policy is
LRU (\emph{least recently used}), which evicts the memory line that was
accessed least recently.
A theoretically useful, but practically impossible policy is the \emph{optimal
replacement policy}, which always evicts the cache line that will be needed
farthest in the future.

Fully associative caches can store an arbitrary set of $m$ cached lines
at any time. Because full associativity would be hard to implement in hardware,
actual caches have a limited associativity. A $t$-way associative
cache only allows each memory location to reside in one of $t$ possible
locations in the cache. If all~$t$~possible locations for a memory line
are already occupied, the cache replacement policy evicts one of the $t$
lines to make space fresh data. In a fully associative cache, we have $t=m$.

The associativity of caches $t$ is usually a small power of two, like 8 or 16.
The set of possible cache locations for lines in memory is determined
by simply masking out a few address bits. This approach implies that
accessing too many locations with addresses differing exactly by a power
of two may result in \emph{cache aliasing} -- the accessed locations will
all fall within the same \emph{set}, and the cache will only be able
to store only $t$ useful cache lines at once, which greatly degrades
performance.

Programs which have \emph{spatial} and \emph{temporal locality of reference}
will perform better on computers with hierarchical memory. Several models have
been developed to benchmark algorithms designed for hierarchical memory.

\section{The external memory model}
The simplest model of memory hierarchies is the \emph{external memory model},
which has only two levels of memory: the fast and small \emph{cache} and the
slow and potentially infinite \emph{disk}.\footnote{%
	We choose this naming to avoid the ambiguous term ``memory'',
	which may mean both ``main memory'' in relation to a disk,
	or an ``external memory'' in relation to a cache.
} The motivation for omitting all other levels is that for practical purposes,
only one gap between memory levels is usually the bottleneck. For example,
the bottleneck in a $10\,\text{TB}$ database would probably be disk I/O,
because only a negligible part of the database would fit into main memory and
lower cache levels. If the time to make a query is $30\,\text{ms}$,
optimizing transfers between L2 and L3 caches would be unlikely to help.

The cache and the disk are divided to blocks of size $B$. The size of the cache 
in bytes is fixed and is denoted $M$ ($M=m\cdot B$).
In the context of main memory, $B$ is the size of a cache line
($64\,\text{B}$).

In the external memory model, we allow free computation on data
stored in the cache. The time complexity is measured in the number of
\emph{block transfers}, which read or write an entire block to the disk.
Algorithms are allowed to explicitly transfer blocks, which corresponds
to the boundary between main memory and disks in computers.
The space complexity is the index of the last overwritten block.
The parameters $B$ and $M$ are known to the program, which allows it to tune
the transfers to the specific hierarchy.

Known results in the external memory model (extended to handle an array
of disks allowing simultaneous access) are comprehensively surveyed in
\cite{em-ads}, including optimal sorting algorithms
(in $\Theta(\frac{N}{B}\log_{M/B}\frac{N}{B})$ block transfers),
algorithms for computational geometry, graph algorithms and dictionary
data structures.

\section{Cache-oblivious algorithms}
Cache-oblivious algorithms run in the external memory model.
Their distinguishing property is that cache-oblivious algorithms
are not allowed access to $M$ and $B$. They may only assume the existence
of a main memory and a cache.
One further assumption required by some cache-oblivious algorithms is
that the cache is \emph{tall}, meaning that it can store more blocks
than a block can store bits ($m=\Omega(B)$).

The algorithm must perform well independent of the values of $M$ and $B$.
On a machine with multiple levels of memory hierarchy, this means that
the algorithm performs well on every cache boundary at the same time.
Thanks to this property, cache-oblivious algorithms need little tuning to the
computer they run on. Interestingly, automatically optimized cache boundaries
include not only the usual L1, L2 and L3 caches, but also the \emph{translation
look-aside buffer} (TLB), which caches the mapping of pages from virtual to
physical memory.

Cache-oblivious algorithms cannot control block transfers between the cache
and the memory: they operate in uniformly addressed memory. This is similar
to real computers: L1, L2 and L3 caches are also out of the program's control.
The cache is, however, assumed to be unrealistically powerful: it is fully
associative and the replacement policy is optimal. Fortunately, fully
associative caches with optimal replacement can be simulated with a
$2\times$ slowdown on $2\times$ larger LRU or FIFO caches
\cite{sleator1985amortized}.
The performance of many cache-oblivious algorithms is only slowed down
by a constant factor when we halve the number of blocks, so cache-oblivious
algorithms can usually be adopted in real world programs with only a~constant
slowdown compared to the model.

Many results from the external memory model have been repeated in the
cache-oblivious model, including optimal sorting in
$\Theta(\frac{N}{B}\log_{M/B}\frac{N}{B})$ block transfers and permuting
in $\Theta(\min\{N,\frac{N}{B}\log_{M/B}\frac{N}{B}\})$ block transfers.
On the other hand, the hiding of cache parameters in the cache-oblivious model
does slightly reduce its strength. No cache-oblivious algorithm can achieve
$\Theta(\frac{N}{B}\log_{M/B}\frac{N}{B})$ block transfers for sorting
without the tall cache assumption \cite{limits-of-co}. Cache-oblivious
algorithms can also permute either in $\Theta(N)$ or in
$\Theta(\frac{N}{B}\log_{M/B}\frac{N}{B})$ block transfers, but it is
impossible for a~cache-oblivious algorithm to pick the faster option.
In chapter \ref{chapter:cob}, we show a~data structure that is a
cache-oblivious equivalent of B-trees.
