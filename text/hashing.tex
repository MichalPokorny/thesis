\chapter{Hash tables}
When implementing an unordered dictionary, hashing is the most common tool.
The idea of hashing is reducing the size of the large key space $K$ to
a smaller \emph{hash space} $H$ via a \emph{hashing function}
$h\mathop{:}K\rightarrow H$.
Let us denote the size of the hash space $M$.
For a key $k$, we call $h(k)$ the \emph{hash} of $k$.

The size of the hash space is selected small enough to allow using hashes
of keys as indices in an array, which we call the \emph{hash table}.
The $i$-th element of the hash table may either be empty, or it may contain
a key-value pair $(k,v)$, where $h(k)=i$.

TODO: figure

As long as all inserted keys have distinct hashes, hash tables are easy:
\textsc{Find}s, \textsc{Insert}s and \textsc{Delete}s all consist of just
hashing the key and performing one operation in the hash table -- it takes just
constant time, so also a constant number of memory transfers.
Unfortunately, if the set of stored keys is not known in advance,
some keys $k_1\neq k_2$ may have the same hash value.
This condition is called a \emph{collision}.

There is a multitude of approaches to resolving collisions.

\begin{itemize}
\item
\emph{Separate chaining} stores the set of all key-value pairs with
$h(k)=i$ in slot $i$. Let us call this set the \emph{collision chain} for hash
$i$, and denote its size $C_i$. The easiest solution is storing a pointer to
a linked list of colliding key-value pairs in the hash table. If there are no
collisions in an occupied slot, an easy and common optimization is storing
the only key-value pair directly in the hash table.

If we use separate chaining, the costs of hash table operations become $\O(1)$
for slot lookup and $\O(|C_i|)$ for scanning the collision chain. By picking
a hash function that evenly distributes hashes among keys, we can prove
that the expected length of a collision chain is short.

\item
When we attempt to \textsc{Insert}a new pair with key $k$ into an occupied slot
$h(k)$, we can start trying out alternate slots $a(k,1), a(k,2), \ldots$
until we succeed in finding an empty slot. When using this family of strategies,
one also needs to slightly change \textsc{Find} and \textsc{Delete}:
\textsc{Find} must properly traverse all possible locations that may contain
the sought key, and \textsc{Delete} must accordingly update any information
\textsc{Find} needs to know when to abort the search.

\emph{Linear probing} simply defines $a(k,x)$ as $(h(k)+x) \bmod M$.
Folk wisdom holds that linear probing is prone to creating long chains
of full slots: if slots $[i;j]$ are full, then every insertion with
$h(k)\in[i;j]$ will extend the length of the full block.

\emph{Quadratic probing} attempts to avoid this problem by instead picking
$a(k,x)$ as $(h(k)+x^2) \bmod M$.

\item
\emph{Perfect hashing} avoids the issue of collisions by picking a hashing
function that is collision-free for the given key set. If we fix the key
set in advance (i.e.\ if we perform \emph{static hashing}), we can use
a variety of algorithms which produce a collision-free hashing function
at the cost of some preprocessing time. If the hash function produces no
empty slots, we call it \emph{minimal}.

For example, HDC (\textit{Hash, Displace and Compress},
\cite{hdc-hashing}) is a randomized algorithm that can generate a perfect
hash function in expected $\O(N)$ time. For $M=1.01 N$, the hash function can be
represented using 1.98 bits per key. All hash functions generated by HDC can be
evaluated in constant time. The algorithm can be simply generalized to build
$k$-perfect hash functions, which allow up to $k$ collisions per slot.

A dynamic version of perfect hashing, commonly referred to as \emph{FKS
hashing} after the initials of the authors, was developed in \cite{fks-hashing}.
TODO: more.

\item
\emph{Cuckoo hashing}
TODO

\end{itemize}

TODO: hash function properties

A hash function $h$ is $k$-independent if the hashes of $k$ different keys
are independent random variables.
