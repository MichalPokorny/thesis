\chapter{Hash tables}
\ref{chapter:hashing}
When implementing an unordered dictionary, hashing is the most common tool.
Hashing is a very old idea and the approaches are numerous. Hashing
techniques usually allow amortized constant-time \textsc{Find}, \textsc{Insert}
and \textsc{Delete} at the expense of disallowing \textsc{FindNext} and
\textsc{FindPrevious}. Certain schemes provide stronger than expected-time
bounds, like deterministic constant time for \textsc{Find}s in cuckoo hashing
(described in section \ref{sec:cuckoo}).

The idea of hashing is reducing the size of the large key universe $U$ to
a smaller \emph{hash space} $H$ via a \emph{hashing function}
$h\mathop{:}U\rightarrow H$.
Let us denote the size of the hash space $M$.
For a key $k$, we call $h(k)$ the \emph{hash} of $k$.

The size of the hash space is selected small enough to allow using hashes
of keys as indices in an array, which we call the \emph{hash table}.
The $i$-th element of the hash table may either be empty, or it may contain
a key-value pair $(k,v)$, where $h(k)=i$.

TODO: figure

As long as all inserted keys have distinct hashes, hash tables are easy:
\textsc{Find}s, \textsc{Insert}s and \textsc{Delete}s all consist of just
hashing the key and performing one operation in the hash table -- it takes just
constant time, so also a constant number of memory transfers.
Unfortunately, if the set of stored keys is not known in advance,
some keys $k_1\neq k_2$ may have the same hash value.
This condition is called a \emph{collision}.

There is a multitude of approaches to resolving collisions.

\begin{itemize}
\item
\emph{Separate chaining} stores the set of all key-value pairs with
$h(k)=i$ in slot $i$. Let us call this set the \emph{collision chain} for hash
$i$, and denote its size $C_i$. The easiest solution is storing a pointer to
a linked list of colliding key-value pairs in the hash table. If there are no
collisions in an occupied slot, an easy and common optimization is storing
the only key-value pair directly in the hash table.

If we use separate chaining, the costs of hash table operations become $\O(1)$
for slot lookup and $\O(|C_i|)$ for scanning the collision chain. By picking
a hash function that evenly distributes hashes among keys, we can prove
that the expected length of a collision chain is short.

To keep the expected chain length low, every time the hash table increases
or decreases in size by a constant factor, we rebuild it with a new $M$
picked to be $\O(N)$.

\item
When we attempt to \textsc{Insert}a new pair with key $k$ into an occupied slot
$h(k)$, we can start trying out alternate slots $a(k,1), a(k,2), \ldots$
until we succeed in finding an empty slot. We call this approach \emph{open
addressing}. When using this family of strategies, one also needs to slightly
change \textsc{Find} and \textsc{Delete}: \textsc{Find} must properly traverse
all possible locations that may contain the sought key, and \textsc{Delete}
must accordingly update any information \textsc{Find} needs to know when to
abort the search.
% TODO: document how exactly in our implementation

\emph{Linear probing} simply defines $a(k,x)$ as $(h(k)+x) \bmod M$.
Folk wisdom holds that linear probing is prone to creating long chains
of full slots: if slots $[i;j]$ are full, then every insertion with
$h(k)\in[i;j]$ will extend the length of the full block.

\emph{Quadratic probing} attempts to avoid this problem by instead picking
$a(k,x)$ as $(h(k)+x^2) \bmod M$.

Finally, \emph{double hashing} augments the choice of alternative slots by using
another hashing function: $a(k,x)$ is defined as $(h(k)+x\cdot h'(k)) \bmod M$,
where $h'(k)$ is required to be nonzero.

TODO: While quadratic probing and double hashing avoid long clusters of occupied
slots, they are not as cache-friendly as linear probing.

\item
\emph{Perfect hashing} avoids the issue of collisions by picking a hashing
function that is collision-free for the given key set. If we fix the key
set in advance (i.e.\ if we perform \emph{static hashing}), we can use
a variety of algorithms which produce a collision-free hashing function
at the cost of some preprocessing time. If the hash function produces no
empty slots, we call it \emph{minimal}.

For example, HDC (\textit{Hash, Displace and Compress},
\cite{hdc-hashing}) is a randomized algorithm that can generate a perfect
hash function in expected $\O(N)$ time. For $M=1.01 N$, the hash function can be
represented using 1.98 bits per key. All hash functions generated by HDC can be
evaluated in constant time. The algorithm can be simply generalized to build
$k$-perfect hash functions, which allow up to $k$ collisions per slot.
The \textit{C Minimum Perfect Hashing Library}, available at
\url{http://cmph.sourceforge.net/}, implements several minimum perfect hashing
algorithms.

A dynamic version of perfect hashing, commonly referred to as \emph{FKS
hashing} after the initials of the authors, was developed in \cite{fks-hashing}.
TODO: more.

\item
\emph{Cuckoo hashing}
TODO

\end{itemize}

\section{Properties of hash functions}
Let us define several properties that we would expect from well-behaved hash
functions. Ideally, our hash function would choose the hash of each key
in an independent random fashion. Unfortunately, storing a random function
would take $\O(N)$ random words (or $N \log M$ bits), which is impractically
large.

To reduce the number of possible hash functions from $M^N$, the set of
considered hash functions is restricted to a smaller family $H$.
Given a family $H$ of hashing functions, we call $H$ \emph{universal} if
$\Pr_{h\in H}[h(x)=h(y)]=\O(\frac{1}{M})$ for any two keys $x$ and $y$.
If this probability equals $\frac{1}{M}$ (as in random hash functions),
$H$ is \emph{strongly universal}. % TODO: Do I need strong universality?

A hash function $h$ is $k$-independent if the hashes of $k$ different keys
are independent random variables. Trivially, random hash functions are
$k$-wise independent for any $k\leq |U|$.
