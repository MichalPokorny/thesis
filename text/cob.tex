\chapter{Cache-oblivious B-trees}
% TODO: assume B > 1

\section{Cache-aware B-trees}
% TODO: usual => good? best?
% TODO: reference to B-tree article
In the external memory model, the usual way of storing sorted dictionaries
is using a B-tree. A B-tree is a generalization of balanced binary search
trees, in which nodes keep up to $b$ key-value pairs in sorted order.
An inner node of a binary search tree with key $X$ contains two pointers
pointing to children with keys $< X$ and $\geq X$. The B-tree generalizes
this by having an inner node with keys $K_1,\ldots K_k$ keeping $k+1$ pointers
to children with keys $< K_1$, $K_1\cdots K_2-1$, \dots, $> K_k$.

The process of finding a key-value pair in a B-tree starts at the root node
and walks down the tree using a generalization of binary search. Insertions
similarly first find the right place to put the new key-value pair, and
then walk back up the tree. If an overfull node (with more than $b$ keys)
is encountered, it is split to two nodes of $b/2$ keys and a pointer to
the new node is inserted into the parent. Deletions employ a similar procedure,
merging underfull nodes (with less than $b/2$ keys) with their siblings.

Thus, updates to the B-tree keep the number of keys within all nodes between
$b/2$ and $b$, so the depth of the tree is kept between $\log_b N$ and
$\log_{b/2} N$. The \textsc{Insert}, \textsc{Delete} and
\textsc{Find} operations therefore run in $\Theta(\log_b N)$ time.
The \textsc{FindNext} and \textsc{FindPrevious} also run in $\Theta(\log_b N)$ time,
but if they are invoked after a previous \textsc{Find}, we can keep a pointer
to the leaf containing the key and accordingly adjust it to find the next
or previous key, which runs in $\O(1)$ amortized time.
Thus, B-trees also allow scanning a contigous range of keys of size $n$
in $\Theta(\log_b N + n)$ time.

If we choose the parameter $b$ to be $\Theta(B)$, we obtain a bound of
$\Theta(1+\log_B N)$ block transfers for all operations, which can
be easily shown to be optimal in the comparison model.
A contigous range of $n$ keys can be
read using $\Theta(\log_B N+n/B)$ block transfers.
Thus, the B-tree has optimal performance in the comparison model
if the block size is known.

The \textit{cache-oblivious B-tree} introduced by \cite{demaine00}
is a data structure which gives similar bounds in a cache-oblivious
setting -- it takes $\Theta(1+\log_B N)$ block transfers to perform
a \textsc{Find}, \textsc{FindNext} or \textsc{FindPrevious} and scanning
a range of $n$ keys also takes $\Theta(1+\log_B N+n/B)$ time.
Both \textsc{Insert}s and \textsc{Delete}s cost
$\Theta(1+\frac{\log^2 N}{B}+\log_B N)$ amortized block transfers.
This matches the performance of B-trees for $B=\Omega(\log N\log\log N)$.
% TODO: unfortunately not common for internal memory

TODO: high-level popis
TODO: indirection and merge/splits

% TODO: better solutions - O(log N/B) atd.

\section{The van Emde Boas layout}
One of the building blocks of the cache-oblivious B-tree is the van Emde Boas
layout (named after the van Emde Boas priority queue, which uses similar ideas).
The van Emde Boas layout is a way of mapping the nodes of a full binary
tree of height $h$ to indices $0\ldots 2^h-2$. Other layouts of full binary
trees include the BFS or DFS order.

The advantage of storing a full binary tree in the van Emde Boas layout
is that is lets us read the sequence of keys from the root to any leaf
using $\Theta(1+\log_B N)$ block transfers, which matches the \textsc{Find}
cost of B-trees without the need to know $B$ beforehand.
In contrast, the same operation would cost $\Theta(1+\log N-\log B)$ block
transfers in the DFS or BFS order.

The van Emde Boas layout is defined recursively. To find the van Emde Boas layout
of a full binary tree of height $h$, we split the tree to bottom subtrees
of height $\lhfloor h-1 \rhfloor$ and one top subtree of height $h - \lhfloor
h-1 \rhfloor$.
The subtrees are recursively aligned in the van Emde Boas layout and then laid
out - first the top tree, then the bottom trees in BFS order. The van Emde Boas
layout of a one-node tree is trivial.

\begin{figure}
\centering
\tikzset{
  veb_node/.style = {align=center, inner sep=0pt, text centered, circle,
	  font=\sffamily, draw=black, text width=1.2em},
  block_l0/.style = {rectangle, draw=black, dashed, inner sep=7pt, draw=gray},
  block_l1/.style = {rectangle, draw=black, densely dotted, thin, inner sep=3pt, draw=gray},
  level/.style={level distance=1.7cm}
}
	%level/.style={sibling distance = 6cm/#1, level distance = 1.3cm}
\begin{tikzpicture}[
	scale=0.7,
	level 1/.style = {sibling distance=9.6cm},
	level 2/.style = {sibling distance=4.6cm},
	level 3/.style = {sibling distance=2.3cm},
	level 4/.style = {sibling distance=1cm},
]
	\node [veb_node] (Node0) {0}
	child{ node [veb_node] (Node1) {1}
		child { node [veb_node] (Node2) {2}
			child { node [veb_node] (Node4) {4}
				child { node [veb_node] (Node5) {5} }
				child { node [veb_node] (Node6) {6} }
			}
			child { node [veb_node] (Node7) {7}
				child { node [veb_node] (Node8) {8} }
				child { node [veb_node] (Node9) {9} }
			}
		}
		child { node [veb_node] (Node3) {3}
			child { node [veb_node] (Node10) {10}
				child { node [veb_node] (Node11) {11} }
				child { node [veb_node] (Node12) {12} }
			}
			child { node [veb_node] (Node13) {13}
				child { node [veb_node] (Node14) {14} }
				child { node [veb_node] (Node15) {15} }
			}
		}
	}
	child{ node [veb_node] (Node16) {16}
		child { node [veb_node] (Node17) {17}
			child { node [veb_node] (Node19) {19}
				child { node [veb_node] (Node20) {20} }
				child { node [veb_node] (Node21) {21} }
			}
			child { node [veb_node] (Node22) {22}
				child { node [veb_node] (Node23) {23} }
				child { node [veb_node] (Node24) {24} }
			}
		}
		child { node [veb_node] (Node18) {18}
			child { node [veb_node] (Node25) {25}
				child { node [veb_node] (Node26) {26} }
				child { node [veb_node] (Node27) {27} }
			}
			child { node [veb_node] (Node28) {28}
				child { node [veb_node] (Node29) {29} }
				child { node [veb_node] (Node30) {30} }
			}
		}
	};

	\node [block_l0,fit=(Node0)] (BlockT) {};
	\node [block_l0,fit=(Node1) (Node5) (Node15)] (BlockB0) {};
	\node [block_l0,fit=(Node16) (Node20) (Node30)] (BlockB1) {};

	\node [block_l1,fit=(Node1) (Node2) (Node3)] (BlockB0T) {};
	\node [block_l1,fit=(Node4) (Node5) (Node6)] (BlockB0B0) {};
	\node [block_l1,fit=(Node7) (Node8) (Node9)] (BlockB0B1) {};
	\node [block_l1,fit=(Node10) (Node11) (Node12)] (BlockB0B2) {};
	\node [block_l1,fit=(Node13) (Node14) (Node15)] (BlockB0B3) {};

	\node [block_l1,fit=(Node16) (Node17) (Node18)] (BlockB1T) {};
	\node [block_l1,fit=(Node19) (Node20) (Node21)] (BlockB1B0) {};
	\node [block_l1,fit=(Node22) (Node23) (Node24)] (BlockB1B1) {};
	\node [block_l1,fit=(Node25) (Node26) (Node27)] (BlockB1B2) {};
	\node [block_l1,fit=(Node28) (Node29) (Node30)] (BlockB1B3) {};
\end{tikzpicture}
% TODO: linearized figure, general figure

\caption{The van Emde Boas layout of a full binary tree of height 5.
Boxes mark recursive applications of the construction. The indices within
every box are contiguous.}
\label{fig:veb_layout_5}
\end{figure}

\begin{theorem}
In a tree stored in the van Emde Boas layout, reading the sequence of nodes
on any root-leaf path costs $\Theta(1+\log_B N)$ block transfers (assuming
every node fits in one block).
\end{theorem}

\begin{proof}
Let us examine the recursive applications of the van Emde Boas construction
for a height $h=\log (N+1)$ and denote the heights of the bottom trees
$h_1, h_2, \ldots$. For example, as seen in Figure \ref{fig:veb_layout_5}, for
$h=5$ we have $h_1=4$, $h_2=2$ and $h_3=1$.

Let us assume that the entire tree doesn't fit in a block of size $B$.
Because a single node fits in one block, there exists a \textit{level of
detail} $i$ such that the $2^{h_i}-1$ nodes in the $i$-th-iteration bottom trees
take up less than $B$ nodes, but bottom trees of the $i-1$-th-iteration bottom
trees take up at least $B$ nodes.

Since $2^{h_i}-1 < B$ and $2^{h_i-1}-1 \geq B$, it follows that $h_i=\O(\log_B)$.
By construction, the path of from the root of the tree to any leaf contains
$\Theta(\log N/h_i)=\Theta(\log_B N)$ bottom trees from the $i$-th iteration.
The path also contains one top tree containing the root which may have a height
different than $h_i$, but the height of this tree is always $\leq h_i$.

Because the $2^{h_i}-1$ nodes of any $i$-th-iteration bottom tree are stored
in a contiguous array, every $i$-th-iteration bottom tree (or top tree)
can be read using $\O(1)$ block transfers. Since every root-to-leaf path
is composed of $\Theta(\log_B N)$ such subtrees, traversing the path
takes $\Theta(1+\log_B N)$ block transfers (the added $\O(1)$ covers
the $B\geq N$ case).
\end{proof}

The van Emde Boas layout thus makes a fine data structure for querying static
data, but we need to combine it with another data structure to allow updates.

\subsection{Efficient implementation concerns}
A useful property of the van Emde Boas layout is that it is fully specified
by the height of the tree, so storing a full binary tree in this layout does
not require keeping pointers -- the positions of left and right children
of a node can be calculated when they are needed. We refer to this property
of the layout as allowing \textit{implicit pointers}.

We will only use the van Emde Boas order to walk from the root node to a leaf.
Given the \textit{van Emde Boas ID} of a node,
we can easily calculate the van Emde Boas IDs of its children by
a recursive procedure. This procedure either returns \textit{internal} node
pointers, referencing actual nodes of the tree, or it returns \textit{external}
indexes, which represent virtual nodes below the leaves, counted from left
to right.

TODO: figure?
TODO: check
\begin{algorithmic}
\Function {GetChildren} {$n$: node van Emde Boas ID, $h$: tree height}
	\If {$h = 1$ and $n = 0$} \Return{external (0,1)} \EndIf

	\State {$h_\downarrow \gets \lhfloor h-1 \rhfloor$} \Comment{Calculate
top and bottom heights}
	\State {$h_\uparrow \gets h-h_\downarrow$}
	\State {$N_\uparrow, N_\downarrow \gets 2^{h_\uparrow}-1,
	2^{h_\downarrow}-1$} \Comment{Calculate top and bottom tree sizes}

	\If {$n <  N_\uparrow$}
		\State $\ell, r \gets$ \Call{GetChildren}{$n$,$h_\uparrow$}
		\If {$\ell$ and $r$ are internal}
			\State \Return{internal ($\ell$,$r$)}
		\Else\Comment{$\ell$ and $r$ they point to bottom tree roots}
			\State \Return{internal
				($N_\uparrow+\ell\cdot N_\downarrow$,
				$N_\uparrow+r\cdot N_\downarrow$)
			}
		\EndIf
	\Else
		\State {$i \gets (n-N_\uparrow) \div N_\downarrow$}
			\Comment{The node $n$ lies within the $i$-th bottom tree.}
		\State {$b \gets N_\uparrow + i\cdot N_\downarrow$}
			\Comment{$b$ is the root of the $i$-th bottom tree.}
		\State $\ell,r\gets$ \Call{GetChildren}{$n-b$, $h_\downarrow$}
		\If {$\ell$ and $r$ are internal}
			\State \Return{internal ($\ell+b$, $r+b$)}
		\Else
			\State {$e \gets 2^{h_\downarrow}$} \Comment{Adjust
				indices by $e$ external nodes per bottom
				tree.}
			\State \Return{external ($\ell+i\cdot e$, $r+i\cdot e$)}
		\EndIf
	\EndIf
\EndFunction
\end{algorithmic}

The cost of this procedure in the cache-oblivious model is $\O(1)$, because
it can be implemented using constant memory by modifying the tail recursion into
a loop. Unfortunately, on a real computer, this is not quite the case --
every call of this procedure performs $\Theta(\log\log N)$ instructions and
repeating this $\Theta(\log N)$ times between the root and a leaf is slow.
Indeed, this calculation of implicit pointers can become the performance
bottleneck of the cache-oblivious B-tree.
This can be slightly alleviated by caching the results for trees of small
height.

As described in \cite{brodal01}, at the low cost of precomputing $\O(h)$
items for every height $h$ of the binary tree, we can perform root-leaf
traversals which take constant time per traversed level.

The main observation is if we select a node and perform the van Emde Boas
recursive construction until the selected node is the root of a bottom tree,
this recursion will progress in the same manner for every node in the same
depth $d$.

For every depth $d\in[2;h]$, let us precompute the size $B[d]$ of
the bottom tree rooted in depth $d$, the size $T[d]$ of the corresponding
top tree and the depth $D[d]$ of the top tree's root. The data takes $\O(\log
N)$ memory and it can be computed in $\O(\log N)$ time by an iterative procedure.

While traversing a root-to-leaf path, we shall maintain the current depth
$d$, the index $i$ of the current node $x$ in BFS order and an array
$Pos[j]$ of van Emde Boas order indices of nodes passed in every depth $j<d$
during this traversal.

As the bits of the BFS index $i$ correspond to left and right turns made during
the traversal, the $\log(T[d]+1)$ least significant bits of $i$ are the
index of the unique bottom tree rooted by the node $x$. Because $T[d]$ is
always in the form $2^k-1$, we can find those bits quickly by calculating
$i \And T[d]$.

Because the current node $x$ is the root of the $(i \And T[d])$-th
bottom tree of size $B[d]$ after a top tree of size $T[d]$ rooted in
$Pos[D[d]]$, it follows that the van Emde Boas index of the current node can be
calculated in $\O(1)$ time as:
$$Pos[d]=Pos[D[d]] + T[d] + (i \And T[d]) \cdot B[d]$$

Our root-to-leaf traversal starts by setting $i\gets 0, d\gets 0, Pos[0]=0$.
Navigation to the left or right child of the current node is performed
by updating the BFS index $i$ (setting it to $2i+1$ or $2i+2$ respectively),
incrementing the depth $d$ and calculating the new $Pos[d]$.
We can also return one level higher by reversing the update of $i$ and
decrementing $d$.

TODO: graf

\section{Ordered file maintenance}
The ordered file maintenance problem is storing an ordered list of $N$ keys
(in the given order) in a contiguous array of size $\O(N)$. The operations
allowed on this structure are \textsc{Insert}(\textit{index},\textit{key}),
which inserts a new key before or after a given index in the array,
and \textsc{Delete}(\textit{index}), which deletes an item at a given index.
We allow the array to contain gaps up to a constant maximum size.
Having constant-size gaps allows scanning a range of $n$ keys using the
optimal $\Theta(n/B)$ scans.

The \textit{packed memory array} is a data structure that maintains an ordered
file. Updates (\textsc{Insert}s and \textsc{Delete}s) of a packed memory array
rewrite a contiguous range of size $\Theta(\log^2 N)$ amortized.
Thanks to the range being contiguous, updates will incur
$\Theta(1+\frac{\log^2 N}{B})$ amortized block transfers.

The packed memory array consists of a physical array. Each position in this
array either contains an element, or it is empty.
The array is divided into \textit{leaf blocks} of constant size $\O(\log N)$.
A \textit{virtual full binary tree} is built above those leaf blocks. Every
node of this binary tree represents the range covering all the leaf blocks below.

To properly define the structure, we need to define some terms. The
\textit{capacity} of a certain range of the array is its size and the
\textit{density} of a range is the number of items present in the range divided
by its capacity.
The data structure maintains certain bounds on the densities of subranges of
the array.

The densities in the leaf blocks are kept between \unichar{"00BC} % VULGAR FRACTION ONE QUARTER
and 1. If an update keeps the density of the leaf block within bounds,
we perform the update by rewriting all the $\Theta(\log N)$ items
of the leaf block.
When a block becomes too sparse or too dense, we walk up the binary
tree until we find a node that fits our density requirements.
Those requirements become stricter as we walk up the binary tree.
In particular, the density of a node of depth $d$ within a tree of height $h$
is kept between $\frac{1}{2}-\frac{1}{4}\frac{d}{h}$ and $\frac{3}{4}+\frac{1}{4}\frac{d}{h}$
\footnote{The constants are arbitrary and control the tradeoff
between memory consumption and time complexity of rebuilding.}.
When we find a node that is \textit{within threshold}, we uniformly redistribute
the items in its range. If no such node exists, we resize the entire structure
by a constant factor, which lets us amortize the costs of this resizing to
a multiplicative constant.

We claim that while this redistribution may reorganize a range of size up
to $\O(N)$, the redistribution of a node puts all nodes below it well within
bounds, so the next reorganization will be needed only after many more updates.
% TODO: how much is "many"?

The search for a node to redistribute is implemented as two interspersed scans,
one extending the scanned range to the left, one to the right. The
redistribution can be done by collecting all items in the array on the left side
of the array using a left-to-right scan, followed by a right-to-left scan
putting items into their final destinations. Thus,
redistributing a range of $K$ items takes $\Theta(1+K/B)$ block transfers.

TODO: figure of redistribution

\begin{theorem}
The block-transfer cost of an update of the packed-memory array
is $\Theta(1+\frac{\log^2 N}{B})$ amortized.
\end{theorem}

\begin{proof}
Suppose we need to redistribute some node $X$ of depth $d$ after performing
an insertion. Since we are redistributing this node, it is within threshold,
but one of its children, say $Y$, is not.
Therefore the density of $X$ is at most
$\frac{3}{4}+\frac{1}{4}\frac{d}{h}$, while the density of $Y$ is more than
$\frac{3}{4}+\frac{1}{4}\frac{d+1}{h}$. After we redistribute the items within $X$,
the density of $Y$ will drop by $\frac{1}{h}$. Thus, if we denote the capacity
of $Y$ as $|Y|$, we will need to insert at least $|Y|/h$ items under any
child of $X$ before we need to rebalance $X$ again.
Thus, we can charge the redistributed interval of size $\O(|X|)$ to the
insertions into $Y$, which gives us amortized $\O(\log N)$
redistribution steps per insertion into $Y$.

Since the node $Y$ has in general $h=\Theta(\log N)$ ancestors, we can amortize
$\Theta(\log^2 N)$ redistribution steps per insertion, which is
$\Theta(1+\frac{\log^2 N}{B})$ in block transfers. The proof of the deletion
cost is analogous.
\end{proof}

TODO: other approaches to ordered file maintenance?

\section{Cache-oblivious B-tree}
The first step of constructing a cache-oblivious B-tree is a bootstrapping
structure, which is a combination of a full binary tree in
the van Emde Boas layout and an ordered file maintenance structure.
The ordered file stores inserted key-value pairs sorted by the key.
Nodes of the tree map to ranges in the ordered file, with leaves mapping
to individual slots and the root mapping to the entire ordered file.
The nodes of the tree contain dictionary keys.
Leaves store the keys stored in their corresponding ordered file slots,
or $\infty$ if the slot is empty. Each internal node stores the minimum
of its subtree.

TODO: figure

\textsc{Find}ing a key in the bootstrapping structure is done by binary search
on the tree. The binary search either finds the ordered file slot which contains
the key-value pair, or it finds an empty slot. Either way, walking down
the tree costs $\Theta(1+\log_B N)$ block transfers.

\textsc{Insert}s and \textsc{Delete}s first walk to the position the key
would normally occupy, using $\Theta(1+\log_B N)$ block transfers. The key
is then inserted into the ordered file, which costs
$\Theta(1+\frac{\log^2 N}{B})$ block transfers. Finally, the tree needs to be
updated to reflect the changed content of the ordered file.

TODO: update analysis: VEB update

This bootstrapping structure matches B-trees in the speed of \textsc{Find},
but updates take time $\Theta(1+\log_B N+\frac{\log^2 N}{B})$, which is
$\frac{\log^2 N}{B}$ more than B-trees.
% TODO: precisely when is this term significant?
This term is significant especially when $B$ is small compared to $\log N$.

To remove this term, we will use this bootstrapping data structure with
\textit{indirection} to make the final cache-oblivious B-tree.

The final \textit{cache-oblivious B-tree} will store key-value pairs
partitioned into \textit{pieces} in disjoint intervals. A piece is an array of
$P=\Theta(\log N)$ slots, which may contain key-value pairs.
The number of key-value pairs stored in pieces is maintained between
$P/4$ and $P$. % TODO: opravdu? ne spis 3/4P? to bych mel overit...
Since pieces are physical arrays, reading or fully rewriting a piece
takes $\Theta(\frac{\log N}{B})$ block transfers.
The $\Theta(N/\log N)$ pieces are stored in the \textit{bootstrapping
structure} as values. The key of each piece is the minimal key it contains.

To \textsc{Find} a key in the cache-oblivious B-tree, we walk down the
boostrapping structure in $\Theta(1+\log_B (N/\log N))$ to find the piece
that may contain the key, and scan it in $\Theta(\frac{\log N}{B})$,
so the time to \textsc{Find} is still $\Theta(1+\log_B N)$.
\textsc{FindNext} and \textsc{FindPrevious} work similarly.

\textsc{Insert}s and \textsc{Delete}s first find the appropriate piece
to update in $\Theta(1+\log_B (N/\log N))$. If the piece can be updated
without getting under $P/4$ or over $P$ items, we rewrite the piece in
$\Theta(\frac{\log N}{B})$, removing or inserting the key. If we changed
the minimum of the piece, we also need to walk up the tree in $\Theta(1+\log_B
(N/\log N))$ to propagate the change.

If the piece is too full, we split the piece into two pieces of size
$P/2$. The splitting is done in $\Theta(\frac{\log N}{B})$ block transfers.
Afterwards, the new piece needs to be inserted to the bootstrapping structure
in $\Theta(\frac{\log^2 (N/\log N)}{B})$. Similarly, if the piece is too sparse,
we merge the piece with one of its neighbours. A neighbour can be found in
$\O(1)$, because the pieces are stored in the packed memory array, which
guarantees gaps of constant size. If the two pieces have more than
$3P/4$ items, we only "borrow" some keys from the neighbour and update
the bootstrapping structure to reflect new piece minima.
If there are less than $3P/4$ items in total, the pieces get merged and one
of them will be deleted from the bootstrapping structure in
$\Theta(\frac{\log^2 (N/\log N)}{B})$.

Thus, updates take $\Theta(1+\log_B N)$ plus $\Theta(\frac{\log^2 N}{B})$
every time we need a split or a merge. However, we don't need splits and
merges too often: splitting a full piece creates two pieces of size $P/2$,
which will be only split or merged again after $\Theta(P)$ more updates.
Merging two pieces yields a piece with between $P/2$ and $3P/4$ items.
This new piece will also be updated only after $\Theta(P)$ more updates.
Thus, we can charge each update $\Theta(\frac{1}{\log N}{\log^2 N}{B})$
time for splits and merges, which gives us $\Theta(1+\log_B N+\frac{\log N}{B})=
\Theta(1+\log_B N)$ amortized time for updates.
