\chapter{Cache-oblivious B-trees}
\label{chapter:cob}
% TODO: assume B > 1
% TODO: usual => good? best?
% TODO: reference to B-tree article
The \textit{cache-oblivious B-tree} introduced by \cite{cobt}
is a data structure which replicates the asymptotic performance of B-trees
in a cache-oblivious setting.
\textsc{Find}s and updates on cache-oblivious B-trees require $\Theta(\log_B N)$
\footnote{Strictly speaking, the bound is $\Theta(1+\log_B N)$
	because we perform $\O(1)$ block transfers even when $B\in\omega(N)$.
	We decide to use the reasonable assumption that
	$B<N$ to simplify some bounds in this section.}
amortized block transfers, which is similar to cache-aware B-trees.
Scanning a range of $t$ keys also takes $\Theta(\log_B N+t/B)$ time.
This matches the performance of B-trees for $B=\Omega(\log N\log\log N)$.
% TODO: unfortunately not common for internal memory

The original cache-oblivious B-tree as described in \cite{cobt} used
weight-balanced B-trees. We implement a simplified version of the structure
introduced in \cite{brodal01}, which is composed of three components.
% TODO: nevadi ze tady jsou data obracene?
The \textit{van Emde Boas layout} of binary trees is a way of cache-obliviously
maintaining full binary trees in memory such that reading or updating any
root-to-leaf path in root-to-leaf or leaf-to-root order takes $\Theta(\log_B N)$
block transfers.
Then, the \textit{packed memory array} lets us store a "cache-oblivious linked
list". The items of this "linked list" will represent key-value pairs.
The packed memory array has fast updates in the amortized sense. Inserting
and deleting items overwrites $\Theta(\frac{\log^2 N}{B})$ amortized memory
blocks.
Finally, we combine the two previous parts with \textit{indirection} to obtain
the desired $\Theta(\log_B N)$ time to \textsc{Find}, \textsc{Insert} and
\textsc{Delete}.

\section{The van Emde Boas layout}
The van Emde Boas layout is a way of mapping the nodes of a full binary
tree of height $h$ to indices $0\ldots 2^h-2$. Other layouts of full binary
trees include the BFS or DFS order.

TODO: reference na van emde boas layout clanek

Surprisingly, the van Emde Boas layout was not invented by van Emde Boas.
The name comes from the earlier \textit{van Emde Boas priority queue},
which was invented by \citeauthor{van-emde-boas} in \citeyear{van-emde-boas}.
The van Emde Boas priority queue uses a height splitting scheme which inspired
the van Emde Boas layout.

The advantage of storing a full binary tree in the van Emde Boas layout
is that is lets us read the sequence of keys from the root to any leaf
using $\Theta(\log_B N)$ block transfers, which matches the \textsc{Find}
cost of B-trees without the need to know $B$ beforehand.
In contrast, the same operation would cost $\Theta(\log N-\log B)$ block
transfers in the DFS or BFS order.

The van Emde Boas layout is defined recursively. To find the van Emde Boas layout
of a full binary tree of height $h$, we split the tree to bottom subtrees
of height $\lhfloor h-1 \rhfloor$ and one top subtree of height $h - \lhfloor
h-1 \rhfloor$.
The subtrees are recursively aligned in the van Emde Boas layout and then laid
out: the top tree first, followed by the bottom trees in BFS order.
The van Emde Boas layout of a one-node tree is trivial.

\begin{figure}
\centering
\tikzset{
  veb_node/.style = {align=center, inner sep=0pt, text centered, circle,
	  font=\sffamily, draw=black, text width=1.2em},
  block_l0/.style = {rectangle, draw=black, dashed, inner sep=7pt, draw=gray},
  block_l1/.style = {rectangle, draw=black, densely dotted, thin, inner sep=3pt, draw=gray},
  level/.style={level distance=1.7cm}
}
	%level/.style={sibling distance = 6cm/#1, level distance = 1.3cm}
\begin{tikzpicture}[
	scale=0.7,
	level 1/.style = {sibling distance=9.6cm},
	level 2/.style = {sibling distance=4.6cm},
	level 3/.style = {sibling distance=2.3cm},
	level 4/.style = {sibling distance=1cm},
]
	\node [veb_node] (Node0) {0}
	child{ node [veb_node] (Node1) {1}
		child { node [veb_node] (Node2) {2}
			child { node [veb_node] (Node4) {4}
				child { node [veb_node] (Node5) {5} }
				child { node [veb_node] (Node6) {6} }
			}
			child { node [veb_node] (Node7) {7}
				child { node [veb_node] (Node8) {8} }
				child { node [veb_node] (Node9) {9} }
			}
		}
		child { node [veb_node] (Node3) {3}
			child { node [veb_node] (Node10) {10}
				child { node [veb_node] (Node11) {11} }
				child { node [veb_node] (Node12) {12} }
			}
			child { node [veb_node] (Node13) {13}
				child { node [veb_node] (Node14) {14} }
				child { node [veb_node] (Node15) {15} }
			}
		}
	}
	child{ node [veb_node] (Node16) {16}
		child { node [veb_node] (Node17) {17}
			child { node [veb_node] (Node19) {19}
				child { node [veb_node] (Node20) {20} }
				child { node [veb_node] (Node21) {21} }
			}
			child { node [veb_node] (Node22) {22}
				child { node [veb_node] (Node23) {23} }
				child { node [veb_node] (Node24) {24} }
			}
		}
		child { node [veb_node] (Node18) {18}
			child { node [veb_node] (Node25) {25}
				child { node [veb_node] (Node26) {26} }
				child { node [veb_node] (Node27) {27} }
			}
			child { node [veb_node] (Node28) {28}
				child { node [veb_node] (Node29) {29} }
				child { node [veb_node] (Node30) {30} }
			}
		}
	};

	\node [block_l0,fit=(Node0)] (BlockT) {};
	\node [block_l0,fit=(Node1) (Node5) (Node15)] (BlockB0) {};
	\node [block_l0,fit=(Node16) (Node20) (Node30)] (BlockB1) {};

	\node [block_l1,fit=(Node1) (Node2) (Node3)] (BlockB0T) {};
	\node [block_l1,fit=(Node4) (Node5) (Node6)] (BlockB0B0) {};
	\node [block_l1,fit=(Node7) (Node8) (Node9)] (BlockB0B1) {};
	\node [block_l1,fit=(Node10) (Node11) (Node12)] (BlockB0B2) {};
	\node [block_l1,fit=(Node13) (Node14) (Node15)] (BlockB0B3) {};

	\node [block_l1,fit=(Node16) (Node17) (Node18)] (BlockB1T) {};
	\node [block_l1,fit=(Node19) (Node20) (Node21)] (BlockB1B0) {};
	\node [block_l1,fit=(Node22) (Node23) (Node24)] (BlockB1B1) {};
	\node [block_l1,fit=(Node25) (Node26) (Node27)] (BlockB1B2) {};
	\node [block_l1,fit=(Node28) (Node29) (Node30)] (BlockB1B3) {};
\end{tikzpicture}
% TODO: linearized figure, general figure

\caption{The van Emde Boas layout of a full binary tree of height 5.
Boxes mark recursive applications of the construction. Note that the indices
within every box are contiguous, so at some level of detail, reading
a box will take $\O(1)$ block transfers.}
\label{fig:veb_layout_5}
\end{figure}

\begin{theorem}
In a tree stored in the van Emde Boas layout, reading the sequence of nodes
on any root-leaf path costs $\Theta(\log_B N)$ block transfers.
\end{theorem}

\begin{proof}
Let us examine the recursive applications of the van Emde Boas construction
for a height $h=\log (N+1)$. Denote the heights of the bottom trees
$h_1, h_2, \ldots$. For example, as seen in Figure \ref{fig:veb_layout_5}, for
$h=5$ we have $h_1=4$, $h_2=2$ and $h_3=1$.

Let us assume that the entire tree doesn't fit in a block of size $B$.
Because we can assume that a single node fits in one block, there exists a \textit{level of
detail} $i$ such that the $2^{h_i}-1$ nodes in $i$-th-iteration bottom trees
fit in a block of size $B$, while bottom trees of $i-1$-th-iteration bottom
trees do not.

Since $2^{h_i}-1 < B$ and $2^{h_i-1}-1 \geq B$, it follows that $h_i=\O(\log_B N)$.
By construction, the path of from the root of the tree to any leaf contains
$\Theta(\log N/h_i)=\Theta(\log_B N)$ bottom trees from the $i$-th iteration.
The path also contains one top tree, which includes the root.
As seen on Figure \ref{fig:veb_layout_5}, this top tree may have a height
different than $h_i$, but the height of this tree is always $\leq h_i$.

Because the $2^{h_i}-1$ nodes of any $i$-th-iteration bottom tree are stored
in contiguous memory, every $i$-th-iteration bottom tree (or top tree)
can be read using $\O(1)$ block transfers. Since every root-to-leaf path
is composed of $\Theta(\log_B N)$ such subtrees, traversing the path
takes $\Theta(\log_B N)$ block transfers (plus $\O(1)$ in the uncommon
case when $B > N$).
\end{proof}

The van Emde Boas layout thus makes a fine data structure for querying static
data, but it does not allow inserting or deleting nodes. We will need
to combine it with another data structure to allow updates.

\subsection{Efficient implementation of implicit pointers}
A useful property of the van Emde Boas layout is that it is fully specified
by $h$ of the tree, so there is no need to keep pointers to child nodes.
The positions of left and right children of a node can be calculated from
$h$ when they are needed.
This is particularly useful when $B$ is small and when the size of pointers
is similar to the size of keys, because not storing pointers lets us
effectively use a higher branching factor than B-trees with explicit
pointers could.
For example, if we store 8-byte keys in a B-tree and if we align nodes to
a 64-byte cache line, each node can only store up to 3 keys. In contrast,
64 bytes of a van Emde Boas layout store 8 keys.
% We refer to this property of the layout as allowing
% \textit{implicit pointers}.

We will only use the van Emde Boas order to walk in the direction from
the root node to a leaf or in reverse.
Given the \textit{van Emde Boas ID} of a node (denoted as numbers
inside the nodes in Figure \ref{fig:veb_layout_5}),
we can easily calculate the van Emde Boas IDs of its children by
a recursive procedure. This procedure either returns \textit{internal} node
pointers, referencing actual nodes of the tree, or it returns \textit{external}
indexes, which represent virtual nodes below the leaves, counted from left
to right.

\begin{algorithmic}
\Function {GetChildren} {$n$: node van Emde Boas ID, $h$: tree height}
	\If {$h = 1$ and $n = 0$} \Return{external (0,1)} \EndIf

	\State {$h_\downarrow \gets \lhfloor h-1 \rhfloor$} \Comment{Calculate
top and bottom heights}
	\State {$h_\uparrow \gets h-h_\downarrow$}
	\State {$N_\uparrow, N_\downarrow \gets 2^{h_\uparrow}-1,
	2^{h_\downarrow}-1$} \Comment{Calculate top and bottom tree sizes}

	\If {$n <  N_\uparrow$}
		\State $\ell, r \gets$ \Call{GetChildren}{$n$,$h_\uparrow$}
		\If {$\ell$ and $r$ are internal}
			\State \Return{internal ($\ell$,$r$)}
		\Else\Comment{$\ell$ and $r$ they point to bottom tree roots}
			\State \Return{internal
				($N_\uparrow+\ell\cdot N_\downarrow$,
				$N_\uparrow+r\cdot N_\downarrow$)
			}
		\EndIf
	\Else
		\State {$i \gets (n-N_\uparrow) \div N_\downarrow$}
			\Comment{The node $n$ lies within the $i$-th bottom tree.}
		\State {$b \gets N_\uparrow + i\cdot N_\downarrow$}
			\Comment{$b$ is the root of the $i$-th bottom tree.}
		\State $\ell,r\gets$ \Call{GetChildren}{$n-b$, $h_\downarrow$}
		\If {$\ell$ and $r$ are internal}
			\State \Return{internal ($\ell+b$, $r+b$)}
		\Else
			\State {$e \gets 2^{h_\downarrow}$} \Comment{Adjust
				indices by $e$ external nodes per bottom
				tree.}
			\State \Return{external ($\ell+i\cdot e$, $r+i\cdot e$)}
		\EndIf
	\EndIf
\EndFunction
\end{algorithmic}

The cost of this procedure in the cache-oblivious model is $\O(1)$, because
it can be implemented using constant memory by modifying the tail recursion into
a loop. Unfortunately, on a real computer, this is not quite the case --
every call of this procedure performs $\Theta(\log\log N)$ instructions and
the cost of calling this procedure $\Theta(\log N)$ times between the root
and a leaf is not negligible compared to the cost of memory transfers.
Indeed, this calculation of implicit pointers can become the performance
bottleneck of the cache-oblivious B-tree.
This can be slightly alleviated by caching the results for trees of small
height, which allows us to stop the recursion early.

As described in \cite{brodal01}, at the low cost of precomputing $\O(h)$
items for every height $h$ of the binary tree, we can perform root-leaf
traversals in constant time per traversed level.

The main observation is that for any node in a fixed depth $d$,
performing the recursive construction until the selected node is the root
of a bottom tree will progress through the same sizes of top and bottom trees.

For every depth $d\in[2;h]$, let us precompute the size $B[d]$ of
bottom trees rooted in depth $d$, the size $T[d]$ of the corresponding
top tree and the depth $D[d]$ of the top tree's root. The data takes $\O(\log
N)$ memory and it can be computed in $\O(\log N)$ time by an iterative procedure.
Table \ref{tab:depth_data_example} shows the values of these arrays
for the 5-level van Emde Boas layout shown in figure \ref{fig:veb_layout_5}.

\begin{table}[h]
	\centering
	\begin{tabular}{r|r|r|r}
		$d$ & $B[d]$ & $T[d]$ & $D[d]$ \\
		\hline
		0   & --     & --     & --     \\
		1   & 15     & 1      & 0      \\
		2   & 1      & 1      & 1      \\
		3   & 3      & 3      & 1      \\
		4   & 1      & 1      & 3
	\end{tabular}
	\caption{$B[d]$, $T[d]$ and $D[d]$ for the 5-level van Emde
	Boas layout.}
	\label{tab:depth_data_example}
\end{table}

While traversing a root-to-leaf path, we shall maintain the depth
$d$ of the current node $X$, the index $i$ of the current node in BFS order
and an array $Pos[j]$ of van Emde Boas order indices of nodes passed in every
depth $j<d$ during this traversal.

As the bits of the BFS index $i$ correspond to left and right turns made during
the traversal, the $\log(T[d]+1)$ least significant bits of $i$ are the
index of the unique bottom tree rooted by the node $X$. Because $T[d]$ is
always in the form $2^k-1$, we can find those bits quickly by calculating
$i \And T[d]$.

Because the current node $X$ is the root of the $(i \And T[d])$-th
bottom tree of size $B[d]$ after a top tree of size $T[d]$ rooted in
$Pos[D[d]]$, it follows that the van Emde Boas index of the current node can be
calculated in $\O(1)$ time as:
$$Pos[d]=Pos[D[d]] + T[d] + (i \And T[d]) \cdot B[d]$$

Our root-to-leaf traversal starts by setting $i\gets 0, d\gets 0, Pos[0]=0$.
Navigation to the left or right child of the current node is performed
by updating the BFS index $i$ (setting it to $2i+1$ or $2i+2$ respectively),
incrementing the depth $d$ and calculating the new $Pos[d]$.
We can also return one level higher by reversing the update of $i$ and
decrementing $d$.

In our root-to-leaf traversals, reading the value of a non-leaf node $N$ will be
followed by reading the value of its right child $R$. Based on the value in
$R$, we will then either descend further below $R$, or we will descent into
the subtree below $N$'s left child $L$.
We slightly optimize this access pattern by calculating $Pos[d]$ for $L$
from $Pos[d]$ for $R$ by simply subtracting $T[d]$, because $B[d]$ will
decrease by 1. This saves us one relatively expensive \texttt{imulq}
instruction whenever moving to the left sibling.

\begin{figure}
\centering
\includegraphics{img/veb-drilldown-speed}
\caption{Average time for computing node pointers in root-to-leaf traversals
	of van Emde Boas layouts, implemented trivially and with precomputed
	level data.
	% TODO: data generated by bin/experiments/veb_performance
}
\label{fig:veb_drilldown_speed}
\end{figure}
As evident in figure \ref{fig:veb_drilldown_speed}, using precomputed level data
saves a considerable amount of computation, especially with deeper van Emde Boas
layouts.

\section{Ordered file maintenance}
The ordered file maintenance problem asks us to store an ordered list of
$N$ keys in a given order in a contiguous array of size $\O(N)$.
The allowed operations are \textsc{Insert}(\textit{index},\textit{key}),
which inserts a new key before or after a given index in the array,
and \textsc{Delete}(\textit{index}), which deletes an item at a given index.
We allow the array to contain gaps up to a constant maximum size.
Having constant-size gaps allows scanning a range of $k$ keys using the
optimal $\Theta(k/B)$ scans.

The \textit{packed memory array} is a data structure that maintains an ordered
file. Updates (\textsc{Insert}s and \textsc{Delete}s) of a packed memory array
rewrite a contiguous range of size $\Theta(\log^2 N)$ amortized.
Thanks to the range being contiguous, updates will incur
$\Theta(\frac{\log^2 N}{B})$ amortized block transfers.

The packed memory array consists of a physical array. Each position in this
array either contains an element, or it is empty.
The array is divided into \textit{leaf blocks} of constant size $\O(\log N)$.
A \textit{virtual full binary tree} is built above those leaf blocks. Every
node of this binary tree represents the range covering all the leaf blocks below.
This tree is only conceptual -- it is never stored in memory.

To properly define the structure, we need to define some terms. The
\textit{capacity} of a certain range of the array is its size and the
\textit{density} of a range is the number of items present in the range divided
by its capacity.
The data structure maintains certain bounds on the densities of subranges of
the array.

The densities in the leaf blocks are kept between \unichar{"00BC} % VULGAR FRACTION ONE QUARTER
and 1. If an update keeps the density of the leaf block within thresholds,
we perform the update by rewriting all $\Theta(\log N)$ items of the leaf block.
When a block becomes too sparse or too dense, we walk up the binary
tree until we find a node that fits our density requirements.
Walking up the binary tree corresponds to "multiplying the range by two"
to the left or to the right.

The density requirements become stricter as we walk up the virtual tree.
In particular, the density of a node of depth $d$ within a tree of height $h$
is kept between $\frac{1}{2}-\frac{1}{4}\frac{d}{h}$ and $\frac{3}{4}+\frac{1}{4}\frac{d}{h}$
\footnote{The constants are arbitrary and control the tradeoff
between memory consumption and time complexity of rebuilding.}.
When we find a node that is \textit{within threshold}, we uniformly redistribute
the items in its range. If no such node exists, we resize the entire structure
by a constant factor, which lets us amortize the costs of this resizing to
a multiplicative constant.

We claim that while this redistribution may reorganize a range of size up
to $\O(N)$, the redistribution of a node puts all nodes below it well within
thresholds, so the next reorganization will be needed only after many more
updates.
% TODO: how much is "many"?

The search for a node to redistribute is implemented as two interspersed scans,
one extending the scanned range to the left, one to the right. The
redistribution can be done by collecting all items in the array on the left side
of the array using a left-to-right scan, followed by a right-to-left scan
putting items into their final destinations. Thus,
redistributing a range of $K$ items takes $\Theta(K/B)$ block transfers.

TODO: figure of redistribution

\begin{theorem}
The block-transfer cost of an update of the packed-memory array
is $\Theta(\frac{\log^2 N}{B})$ amortized.
\end{theorem}

\begin{proof}
Suppose we need to redistribute some node $X$ of depth $d$ after performing
an insertion. Since we are redistributing this node, it is within threshold,
but one of its children, say $Y$, is not.
Therefore the density of $X$ is at most
$\frac{3}{4}+\frac{1}{4}\frac{d}{h}$, while the density of $Y$ is more than
$\frac{3}{4}+\frac{1}{4}\frac{d+1}{h}$. After we redistribute the items within $X$,
the density of $Y$ will drop by $\frac{1}{h}$. Thus, if we denote the capacity
of $Y$ as $|Y|$, we will need to insert at least $|Y|/h$ items under any
child of $X$ before we need to rebalance $X$ again.
Thus, we can charge the $\O(|X|)$ cost of redistributing $X$ to the
insertions into $Y$, which gives us amortized $\O(\log N)$
redistribution steps per insertion into $Y$.

Since the node $Y$ has $h=\Theta(\log N)$ ancestors, we can amortize
$\Theta(\log N \cdot \log N)$ redistribution steps per insertion, which is
$\Theta(\frac{\log^2 N}{B})$ in block transfers. The proof of the deletion
cost is analogous.
\end{proof}

\section{Cache-oblivious B-tree}
The first step of constructing a cache-oblivious B-tree is a bootstrapping
structure, which is a combination of a full binary tree in
the van Emde Boas layout and the packed memory array.
The packed memory array stores inserted key-value pairs sorted by the key.
Nodes of the tree map to ranges in the array, with leaves mapping
to individual slots and the root mapping to the entire ordered file.
The nodes of the tree contain dictionary keys.
Leaves store the keys stored in their corresponding ordered file slots,
or $\infty$ if the slot is empty. Each internal node of the tree
stores the minimum of its subtree.

TODO: figure

\textsc{Find}ing a key in the bootstrapping structure is done by binary search
on the tree. The binary search either finds the ordered file slot which contains
the key-value pair, or it finds an empty slot. Either way, walking down
the tree costs $\Theta(\log_B N)$ block transfers.

\textsc{Insert}s and \textsc{Delete}s first walk to the position the key
would normally occupy, using $\Theta(\log_B N)$ block transfers. The key
is then inserted into the ordered file, which costs
$\Theta(\frac{\log^2 N}{B})$ block transfers. Finally, the tree needs to be
updated to reflect the reorganization of the packed memory array.

TODO: update analysis: VEB update

This bootstrapping structure matches B-trees in the speed of \textsc{Find},
but updates take time $\Theta(\log_B N+\frac{\log^2 N}{B})$, which is
$\Theta(\frac{\log^2 N}{B})$ more than B-trees.
% TODO: precisely when is this term significant?
This term is significant especially when $B$ is small compared to $\log N$.

To remove this term, we will use this bootstrapping data structure with
\textit{indirection} to make the final cache-oblivious B-tree.

The final \textit{cache-oblivious B-tree} will store key-value pairs
partitioned into \textit{pieces} in disjoint intervals. A piece is an array of
$P=\Theta(\log N)$ slots, which contains between $P/4$ and $P$ key-value pairs.
% TODO: opravdu? ne spis 3/4P? to bych mel overit...
Since pieces are physical arrays, reading or fully rewriting a piece
takes $\Theta(\frac{\log N}{B})$ block transfers.
The \textit{bootstrapping structure} stores pointers to the $\Theta(N/\log N)$
pieces as values, keyed by their minimal keys.

To \textsc{Find} a key in the cache-oblivious B-tree, we walk down the
boostrapping structure in $\Theta(\log_B (N/\log N))=\Theta(\log_B N)$
to find the piece that may contain the key, and scan it in
$\Theta(\frac{\log N}{B})$, so we still need only $\Theta(\log_B N)$
block transfers for \textsc{Find}s.
\textsc{FindNext} and \textsc{FindPrevious} work similarly.

\textsc{Insert}s and \textsc{Delete}s first find the appropriate piece
to update in $\Theta(\log_B (N/\log N))$. If the piece can be updated
without getting under $P/4$ or over $P$ items, we rewrite the piece in
$\Theta(\frac{\log N}{B})$, removing or inserting the key. If we changed
the minimum of the piece, we also need to walk up the tree in $\Theta(\log_B
(N/\log N))$ to propagate the change.

If the piece is too full, we split the piece into two pieces of size
$P/2$. The splitting takes $\Theta(\frac{\log N}{B})$ block transfers.
Afterwards, the new piece needs to be inserted to the bootstrapping structure
in $\Theta(\frac{\log^2 (N/\log N)}{B})$ amortized transfers.
Similarly, if the piece is too sparse, we merge the piece with one of its
neighbours. A neighbour can be found in $\O(1)$, because the pieces are
stored in the packed memory array, which guarantees gaps of constant size.
If the two pieces have more than $3P/4$ items, we only "borrow" some keys from
the neighbour and update the bootstrapping structure in $\Theta(\log_B N)$
to reflect new piece minima.
If there are less than $3P/4$ items in total, the pieces get merged and one
of them will be deleted from the bootstrapping structure in
$\Theta(\frac{\log^2 (N/\log N)}{B})$.

Thus, updates take $\Theta(\log_B N)$ plus $\Theta(\frac{\log^2 N}{B})$
every time we need a split or a merge. However, we don't need splits and
merges too often: splitting a full piece creates two pieces of size $P/2$,
which will be only split or merged again after $\Theta(P)$ more updates.
Merging two pieces yields a piece with between $P/2$ and $3P/4$ items.
This new piece will also be updated only after $\Theta(P)$ more updates.
Thus, we can charge each update $\Theta(\frac{1}{\log N}\frac{\log^2 N}{B})$
for splits and merges, which gives us $\Theta(\log_B N+\frac{\log N}{B})=
\Theta(\log_B N)$ amortized time for updates.

\section{Enhancements}
An alternative deamortized ordered file maintenance data structure
developed by \cite{willard92} performs $\Theta(\frac{\log^2 N}{B})$
block transfers in the worst case. A cache-oblivious B-tree backed by this
structure would limit the worst-case number of block transfers from
$\Theta(\frac{N}{\log N})$ to just $\Theta(\frac{\log^2 N}{B}+\log_B
N)$ while keeping the amortized bound at $\Theta(\log_B N)$.

% TODO: Should I mention this?
% \footnote{
% 	In a practical implementation, the worst-case bound needs an additional
% 	$\frac{N}{B}$ term to account for necessary reallocations of
% 	the backing array once it becomes too full.}

As we observed with applying the van Emde Boas layout, removing auxilliary
data can greatly speed up operations -- the tree in van Emde Boas layout has
half the effective depth of a B-tree, because cache lines can fit more keys
when pointers are implicit.
\textit{Implicit data structures} are designed to waste as little memory on
auxilliary information as possible. Implicit dictionaries are allowed to only
store a permutation of the stored data and $\O(1)$ memory cells of additional
information. Bookkeeping information, such as pointers or small integers
of $b$ bits, is usually encoded by a pairwise permutation of $2b$ keys,
with $x_{2i} < x_{2i+1}$ corresponding to 1 and $x_{2i} > x_{2i+1}$
corresponding to 0.

\cite{implicit-btrees-survey} gives an overview of progress on implicit
dictionaries.  In \citeyear{implicit-cob}, \citeauthor{implicit-cob} reached
the optimal bound of $\Theta(\log_B N)$ block transfers per search or operation
in an implicit setting. Similarly to the simple cache-oblivious B-tree, their
construction uses the deamortized version of the packed memory array
in conjunction with the van Emde Boas layout. To avoid holes in the packed
memory array, the elements of the packed memory array are chunks of keys.
Several other modifications are necessary to allow updates in size, which
requires building the structure on the fly.
We have decided not to implement the implicit cache-oblivious B-tree due
to its large complexity. We also believe the data structure may be much
slower on practical data sets than equivalent structures with free slots or
pointers, since decoding auxilliary data from the implicit representation
requires reading large permutations of keys.
